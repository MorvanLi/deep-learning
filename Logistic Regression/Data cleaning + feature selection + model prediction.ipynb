{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 特征最影响结果的K个特征\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# 卡方检验，作为SelectKBest的参数\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./datas/titanic/titanic_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0            1         0       3    male  22.0      1      0   7.2500        S\n",
       "1            2         1       1  female  38.0      1      0  71.2833        C\n",
       "2            3         1       3  female  26.0      0      0   7.9250        S\n",
       "3            4         1       1  female  35.0      1      0  53.1000        S\n",
       "4            5         0       3    male  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"PassengerId\", \"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]].copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Sex          891 non-null    object \n",
      " 4   Age          891 non-null    float64\n",
      " 5   SibSp        891 non-null    int64  \n",
      " 6   Parch        891 non-null    int64  \n",
      " 7   Fare         891 non-null    float64\n",
      " 8   Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(2)\n",
      "memory usage: 62.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>830</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass     Sex   Age  SibSp  Parch  Fare Embarked\n",
       "61            62         1       1  female  38.0      0      0  80.0      NaN\n",
       "829          830         1       1  female  62.0      0      0  80.0      NaN"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们可以从上面看到，Age Embarked是有缺失的\n",
    "data[\"Age\"] = data[\"Age\"].fillna(data[\"Age\"].median()) # 给Age列填充平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>830</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass     Sex   Age  SibSp  Parch  Fare Embarked\n",
       "61            62         1       1  female  38.0      0      0  80.0      NaN\n",
       "829          830         1       1  female  62.0      0      0  80.0      NaN"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.isnull().values==True]  # 这是看具体缺失的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Embarked\"] = data[\"Embarked\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked\n",
       "0            1         0       3    male  22.0      1      0   7.2500        S\n",
       "1            2         1       1  female  38.0      1      0  71.2833        C\n",
       "2            3         1       3  female  26.0      0      0   7.9250        S\n",
       "3            4         1       1  female  35.0      1      0  53.1000        S\n",
       "4            5         0       3    male  35.0      0      0   8.0500        S"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S', 'C', 'Q', 0], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "data.Embarked.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[\"Embarked\"] == \"S\", \"Embarked\"] = 1\n",
    "data.loc[data[\"Embarked\"] == \"C\", \"Embarked\"] = 2\n",
    "data.loc[data[\"Embarked\"] == \"Q\", \"Embarked\"] = 3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4、将特征列和结果列拆分开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data.pop(\"Survived\")\n",
    "X = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5、使用卡方检验选择topK的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择所有的特征，目的是看到特征重要性排序\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=len(X.columns))\n",
    "fit = bestfeatures.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.312934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.873699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>170.348127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.649163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.581865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.097499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4518.319091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.771019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0     3.312934\n",
       "1    30.873699\n",
       "2   170.348127\n",
       "3    21.649163\n",
       "4     2.581865\n",
       "5    10.097499\n",
       "6  4518.319091\n",
       "7     2.771019"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores = pd.DataFrame(fit.scores_)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PassengerId</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pclass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SibSp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Parch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Embarked</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0  PassengerId\n",
       "1       Pclass\n",
       "2          Sex\n",
       "3          Age\n",
       "4        SibSp\n",
       "5        Parch\n",
       "6         Fare\n",
       "7     Embarked"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_columns = pd.DataFrame(X.columns)\n",
    "df_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PassengerId</td>\n",
       "      <td>3.312934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pclass</td>\n",
       "      <td>30.873699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "      <td>170.348127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Age</td>\n",
       "      <td>21.649163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SibSp</td>\n",
       "      <td>2.581865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Parch</td>\n",
       "      <td>10.097499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fare</td>\n",
       "      <td>4518.319091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Embarked</td>\n",
       "      <td>2.771019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature_name        Score\n",
       "0  PassengerId     3.312934\n",
       "1       Pclass    30.873699\n",
       "2          Sex   170.348127\n",
       "3          Age    21.649163\n",
       "4        SibSp     2.581865\n",
       "5        Parch    10.097499\n",
       "6         Fare  4518.319091\n",
       "7     Embarked     2.771019"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 合并两个df\n",
    "df_feature_scores = pd.concat([df_columns,df_scores],axis=1)\n",
    "# 列名\n",
    "df_feature_scores.columns = ['feature_name','Score']  #naming the dataframe columns\n",
    "\n",
    "# 查看\n",
    "df_feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_name</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fare</td>\n",
       "      <td>4518.319091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sex</td>\n",
       "      <td>170.348127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pclass</td>\n",
       "      <td>30.873699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Age</td>\n",
       "      <td>21.649163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Parch</td>\n",
       "      <td>10.097499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PassengerId</td>\n",
       "      <td>3.312934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Embarked</td>\n",
       "      <td>2.771019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SibSp</td>\n",
       "      <td>2.581865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature_name        Score\n",
       "6         Fare  4518.319091\n",
       "2          Sex   170.348127\n",
       "1       Pclass    30.873699\n",
       "3          Age    21.649163\n",
       "5        Parch    10.097499\n",
       "0  PassengerId     3.312934\n",
       "7     Embarked     2.771019\n",
       "4        SibSp     2.581865"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_scores.sort_values(by=\"Score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71.283302</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.925000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.099998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.050000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Fare  Sex  Pclass   Age  Parch\n",
       "0   7.250000  0.0     3.0  22.0    0.0\n",
       "1  71.283302  1.0     1.0  38.0    0.0\n",
       "2   7.925000  1.0     3.0  26.0    0.0\n",
       "3  53.099998  1.0     1.0  35.0    0.0\n",
       "4   8.050000  0.0     3.0  35.0    0.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们选择前5个特征\n",
    "data = X[['Fare', 'Sex', 'Pclass', 'Age', 'Parch']].astype(np.float32)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(data.values).type(torch.float32)\n",
    "Y = Y.type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(5, 1),\n",
    "    nn.Sigmoid()\n",
    " )\n",
    "loss_fn = nn.BCELoss()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, Y)\n",
    "\n",
    "train_ds = TensorDataset(train_x, train_y)\n",
    "train_dl = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "test_ds = TensorDataset(test_x, test_y)\n",
    "test_dl = DataLoader(test_ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 loss:  2.81 acc_train: 0.298 acc_test: 0.345\n",
      "epoch:  1 loss:  0.828 acc_train: 0.66 acc_test: 0.641\n",
      "epoch:  2 loss:  0.738 acc_train: 0.573 acc_test: 0.556\n",
      "epoch:  3 loss:  1.331 acc_train: 0.621 acc_test: 0.623\n",
      "epoch:  4 loss:  0.606 acc_train: 0.63 acc_test: 0.614\n",
      "epoch:  5 loss:  0.622 acc_train: 0.597 acc_test: 0.587\n",
      "epoch:  6 loss:  0.515 acc_train: 0.624 acc_test: 0.592\n",
      "epoch:  7 loss:  0.595 acc_train: 0.576 acc_test: 0.552\n",
      "epoch:  8 loss:  0.765 acc_train: 0.635 acc_test: 0.65\n",
      "epoch:  9 loss:  0.586 acc_train: 0.644 acc_test: 0.637\n",
      "epoch:  10 loss:  0.707 acc_train: 0.603 acc_test: 0.587\n",
      "epoch:  11 loss:  0.701 acc_train: 0.626 acc_test: 0.587\n",
      "epoch:  12 loss:  0.508 acc_train: 0.624 acc_test: 0.596\n",
      "epoch:  13 loss:  0.542 acc_train: 0.615 acc_test: 0.583\n",
      "epoch:  14 loss:  0.541 acc_train: 0.639 acc_test: 0.637\n",
      "epoch:  15 loss:  0.66 acc_train: 0.635 acc_test: 0.637\n",
      "epoch:  16 loss:  1.018 acc_train: 0.659 acc_test: 0.655\n",
      "epoch:  17 loss:  0.605 acc_train: 0.633 acc_test: 0.601\n",
      "epoch:  18 loss:  0.684 acc_train: 0.647 acc_test: 0.641\n",
      "epoch:  19 loss:  0.61 acc_train: 0.632 acc_test: 0.601\n",
      "epoch:  20 loss:  1.091 acc_train: 0.656 acc_test: 0.641\n",
      "epoch:  21 loss:  0.758 acc_train: 0.657 acc_test: 0.641\n",
      "epoch:  22 loss:  0.899 acc_train: 0.674 acc_test: 0.628\n",
      "epoch:  23 loss:  0.947 acc_train: 0.642 acc_test: 0.641\n",
      "epoch:  24 loss:  0.702 acc_train: 0.66 acc_test: 0.641\n",
      "epoch:  25 loss:  0.707 acc_train: 0.663 acc_test: 0.632\n",
      "epoch:  26 loss:  0.722 acc_train: 0.668 acc_test: 0.628\n",
      "epoch:  27 loss:  0.711 acc_train: 0.663 acc_test: 0.65\n",
      "epoch:  28 loss:  0.825 acc_train: 0.674 acc_test: 0.646\n",
      "epoch:  29 loss:  0.703 acc_train: 0.681 acc_test: 0.628\n",
      "epoch:  30 loss:  0.771 acc_train: 0.663 acc_test: 0.646\n",
      "epoch:  31 loss:  0.534 acc_train: 0.635 acc_test: 0.605\n",
      "epoch:  32 loss:  0.591 acc_train: 0.635 acc_test: 0.628\n",
      "epoch:  33 loss:  0.653 acc_train: 0.674 acc_test: 0.632\n",
      "epoch:  34 loss:  0.835 acc_train: 0.656 acc_test: 0.614\n",
      "epoch:  35 loss:  0.672 acc_train: 0.665 acc_test: 0.646\n",
      "epoch:  36 loss:  0.595 acc_train: 0.669 acc_test: 0.646\n",
      "epoch:  37 loss:  0.596 acc_train: 0.647 acc_test: 0.641\n",
      "epoch:  38 loss:  0.802 acc_train: 0.657 acc_test: 0.61\n",
      "epoch:  39 loss:  0.698 acc_train: 0.672 acc_test: 0.637\n",
      "epoch:  40 loss:  0.997 acc_train: 0.635 acc_test: 0.641\n",
      "epoch:  41 loss:  1.135 acc_train: 0.675 acc_test: 0.646\n",
      "epoch:  42 loss:  0.464 acc_train: 0.675 acc_test: 0.641\n",
      "epoch:  43 loss:  0.589 acc_train: 0.671 acc_test: 0.641\n",
      "epoch:  44 loss:  0.609 acc_train: 0.689 acc_test: 0.632\n",
      "epoch:  45 loss:  0.77 acc_train: 0.644 acc_test: 0.637\n",
      "epoch:  46 loss:  0.702 acc_train: 0.648 acc_test: 0.632\n",
      "epoch:  47 loss:  0.655 acc_train: 0.674 acc_test: 0.65\n",
      "epoch:  48 loss:  0.535 acc_train: 0.672 acc_test: 0.65\n",
      "epoch:  49 loss:  0.537 acc_train: 0.669 acc_test: 0.655\n",
      "epoch:  50 loss:  0.686 acc_train: 0.68 acc_test: 0.646\n",
      "epoch:  51 loss:  0.628 acc_train: 0.678 acc_test: 0.641\n",
      "epoch:  52 loss:  0.674 acc_train: 0.642 acc_test: 0.641\n",
      "epoch:  53 loss:  0.606 acc_train: 0.672 acc_test: 0.65\n",
      "epoch:  54 loss:  0.522 acc_train: 0.624 acc_test: 0.641\n",
      "epoch:  55 loss:  0.629 acc_train: 0.671 acc_test: 0.65\n",
      "epoch:  56 loss:  0.43 acc_train: 0.656 acc_test: 0.632\n",
      "epoch:  57 loss:  0.672 acc_train: 0.686 acc_test: 0.646\n",
      "epoch:  58 loss:  0.416 acc_train: 0.65 acc_test: 0.646\n",
      "epoch:  59 loss:  0.766 acc_train: 0.687 acc_test: 0.637\n",
      "epoch:  60 loss:  0.684 acc_train: 0.695 acc_test: 0.632\n",
      "epoch:  61 loss:  0.654 acc_train: 0.683 acc_test: 0.65\n",
      "epoch:  62 loss:  0.652 acc_train: 0.689 acc_test: 0.646\n",
      "epoch:  63 loss:  0.511 acc_train: 0.671 acc_test: 0.65\n",
      "epoch:  64 loss:  1.068 acc_train: 0.659 acc_test: 0.641\n",
      "epoch:  65 loss:  0.676 acc_train: 0.686 acc_test: 0.637\n",
      "epoch:  66 loss:  0.508 acc_train: 0.663 acc_test: 0.646\n",
      "epoch:  67 loss:  0.823 acc_train: 0.671 acc_test: 0.65\n",
      "epoch:  68 loss:  0.593 acc_train: 0.663 acc_test: 0.65\n",
      "epoch:  69 loss:  0.585 acc_train: 0.69 acc_test: 0.632\n",
      "epoch:  70 loss:  0.495 acc_train: 0.66 acc_test: 0.646\n",
      "epoch:  71 loss:  0.452 acc_train: 0.66 acc_test: 0.65\n",
      "epoch:  72 loss:  0.552 acc_train: 0.657 acc_test: 0.655\n",
      "epoch:  73 loss:  0.521 acc_train: 0.672 acc_test: 0.65\n",
      "epoch:  74 loss:  0.777 acc_train: 0.687 acc_test: 0.641\n",
      "epoch:  75 loss:  0.505 acc_train: 0.686 acc_test: 0.641\n",
      "epoch:  76 loss:  0.51 acc_train: 0.647 acc_test: 0.659\n",
      "epoch:  77 loss:  0.523 acc_train: 0.62 acc_test: 0.65\n",
      "epoch:  78 loss:  0.765 acc_train: 0.69 acc_test: 0.623\n",
      "epoch:  79 loss:  0.737 acc_train: 0.665 acc_test: 0.65\n",
      "epoch:  80 loss:  0.447 acc_train: 0.669 acc_test: 0.65\n",
      "epoch:  81 loss:  0.622 acc_train: 0.687 acc_test: 0.641\n",
      "epoch:  82 loss:  0.622 acc_train: 0.668 acc_test: 0.664\n",
      "epoch:  83 loss:  0.759 acc_train: 0.674 acc_test: 0.65\n",
      "epoch:  84 loss:  0.61 acc_train: 0.684 acc_test: 0.646\n",
      "epoch:  85 loss:  0.56 acc_train: 0.687 acc_test: 0.646\n",
      "epoch:  86 loss:  0.613 acc_train: 0.678 acc_test: 0.65\n",
      "epoch:  87 loss:  0.539 acc_train: 0.678 acc_test: 0.659\n",
      "epoch:  88 loss:  0.57 acc_train: 0.668 acc_test: 0.655\n",
      "epoch:  89 loss:  0.595 acc_train: 0.674 acc_test: 0.673\n",
      "epoch:  90 loss:  0.467 acc_train: 0.677 acc_test: 0.659\n",
      "epoch:  91 loss:  0.544 acc_train: 0.675 acc_test: 0.655\n",
      "epoch:  92 loss:  0.667 acc_train: 0.639 acc_test: 0.659\n",
      "epoch:  93 loss:  0.56 acc_train: 0.678 acc_test: 0.664\n",
      "epoch:  94 loss:  0.682 acc_train: 0.617 acc_test: 0.628\n",
      "epoch:  95 loss:  0.56 acc_train: 0.687 acc_test: 0.664\n",
      "epoch:  96 loss:  0.482 acc_train: 0.686 acc_test: 0.646\n",
      "epoch:  97 loss:  0.527 acc_train: 0.686 acc_test: 0.677\n",
      "epoch:  98 loss:  0.773 acc_train: 0.675 acc_test: 0.668\n",
      "epoch:  99 loss:  0.876 acc_train: 0.629 acc_test: 0.65\n",
      "epoch:  100 loss:  0.536 acc_train: 0.678 acc_test: 0.682\n",
      "epoch:  101 loss:  0.788 acc_train: 0.675 acc_test: 0.673\n",
      "epoch:  102 loss:  0.678 acc_train: 0.677 acc_test: 0.664\n",
      "epoch:  103 loss:  0.527 acc_train: 0.681 acc_test: 0.655\n",
      "epoch:  104 loss:  0.655 acc_train: 0.692 acc_test: 0.65\n",
      "epoch:  105 loss:  0.555 acc_train: 0.695 acc_test: 0.637\n",
      "epoch:  106 loss:  0.538 acc_train: 0.678 acc_test: 0.673\n",
      "epoch:  107 loss:  0.555 acc_train: 0.686 acc_test: 0.673\n",
      "epoch:  108 loss:  0.579 acc_train: 0.68 acc_test: 0.664\n",
      "epoch:  109 loss:  0.605 acc_train: 0.686 acc_test: 0.637\n",
      "epoch:  110 loss:  0.652 acc_train: 0.681 acc_test: 0.677\n",
      "epoch:  111 loss:  0.594 acc_train: 0.692 acc_test: 0.655\n",
      "epoch:  112 loss:  0.555 acc_train: 0.686 acc_test: 0.668\n",
      "epoch:  113 loss:  0.584 acc_train: 0.668 acc_test: 0.673\n",
      "epoch:  114 loss:  0.526 acc_train: 0.666 acc_test: 0.668\n",
      "epoch:  115 loss:  0.737 acc_train: 0.689 acc_test: 0.682\n",
      "epoch:  116 loss:  0.538 acc_train: 0.689 acc_test: 0.655\n",
      "epoch:  117 loss:  0.438 acc_train: 0.636 acc_test: 0.659\n",
      "epoch:  118 loss:  0.564 acc_train: 0.686 acc_test: 0.673\n",
      "epoch:  119 loss:  0.695 acc_train: 0.693 acc_test: 0.659\n",
      "epoch:  120 loss:  0.825 acc_train: 0.693 acc_test: 0.655\n",
      "epoch:  121 loss:  0.479 acc_train: 0.69 acc_test: 0.65\n",
      "epoch:  122 loss:  0.618 acc_train: 0.638 acc_test: 0.664\n",
      "epoch:  123 loss:  0.359 acc_train: 0.687 acc_test: 0.686\n",
      "epoch:  124 loss:  0.955 acc_train: 0.689 acc_test: 0.655\n",
      "epoch:  125 loss:  0.518 acc_train: 0.69 acc_test: 0.673\n",
      "epoch:  126 loss:  0.645 acc_train: 0.684 acc_test: 0.668\n",
      "epoch:  127 loss:  0.571 acc_train: 0.687 acc_test: 0.655\n",
      "epoch:  128 loss:  0.63 acc_train: 0.689 acc_test: 0.673\n",
      "epoch:  129 loss:  0.394 acc_train: 0.695 acc_test: 0.673\n",
      "epoch:  130 loss:  0.569 acc_train: 0.693 acc_test: 0.668\n",
      "epoch:  131 loss:  0.702 acc_train: 0.692 acc_test: 0.659\n",
      "epoch:  132 loss:  0.624 acc_train: 0.698 acc_test: 0.691\n",
      "epoch:  133 loss:  0.643 acc_train: 0.696 acc_test: 0.637\n",
      "epoch:  134 loss:  0.648 acc_train: 0.669 acc_test: 0.673\n",
      "epoch:  135 loss:  0.607 acc_train: 0.695 acc_test: 0.659\n",
      "epoch:  136 loss:  0.45 acc_train: 0.695 acc_test: 0.695\n",
      "epoch:  137 loss:  0.508 acc_train: 0.711 acc_test: 0.646\n",
      "epoch:  138 loss:  0.808 acc_train: 0.674 acc_test: 0.677\n",
      "epoch:  139 loss:  0.596 acc_train: 0.695 acc_test: 0.668\n",
      "epoch:  140 loss:  0.571 acc_train: 0.698 acc_test: 0.664\n",
      "epoch:  141 loss:  0.483 acc_train: 0.693 acc_test: 0.677\n",
      "epoch:  142 loss:  0.629 acc_train: 0.702 acc_test: 0.668\n",
      "epoch:  143 loss:  0.544 acc_train: 0.702 acc_test: 0.7\n",
      "epoch:  144 loss:  0.599 acc_train: 0.699 acc_test: 0.704\n",
      "epoch:  145 loss:  0.627 acc_train: 0.695 acc_test: 0.682\n",
      "epoch:  146 loss:  0.569 acc_train: 0.669 acc_test: 0.673\n",
      "epoch:  147 loss:  0.588 acc_train: 0.692 acc_test: 0.682\n",
      "epoch:  148 loss:  0.505 acc_train: 0.699 acc_test: 0.677\n",
      "epoch:  149 loss:  0.472 acc_train: 0.696 acc_test: 0.668\n",
      "epoch:  150 loss:  0.534 acc_train: 0.687 acc_test: 0.695\n",
      "epoch:  151 loss:  0.612 acc_train: 0.69 acc_test: 0.691\n",
      "epoch:  152 loss:  0.567 acc_train: 0.698 acc_test: 0.695\n",
      "epoch:  153 loss:  0.495 acc_train: 0.708 acc_test: 0.664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  154 loss:  0.478 acc_train: 0.701 acc_test: 0.677\n",
      "epoch:  155 loss:  0.506 acc_train: 0.615 acc_test: 0.619\n",
      "epoch:  156 loss:  0.524 acc_train: 0.708 acc_test: 0.646\n",
      "epoch:  157 loss:  0.61 acc_train: 0.69 acc_test: 0.673\n",
      "epoch:  158 loss:  0.633 acc_train: 0.71 acc_test: 0.646\n",
      "epoch:  159 loss:  0.569 acc_train: 0.708 acc_test: 0.686\n",
      "epoch:  160 loss:  0.501 acc_train: 0.701 acc_test: 0.686\n",
      "epoch:  161 loss:  0.477 acc_train: 0.693 acc_test: 0.691\n",
      "epoch:  162 loss:  0.73 acc_train: 0.695 acc_test: 0.673\n",
      "epoch:  163 loss:  0.73 acc_train: 0.71 acc_test: 0.659\n",
      "epoch:  164 loss:  0.521 acc_train: 0.692 acc_test: 0.695\n",
      "epoch:  165 loss:  0.557 acc_train: 0.711 acc_test: 0.655\n",
      "epoch:  166 loss:  0.911 acc_train: 0.711 acc_test: 0.646\n",
      "epoch:  167 loss:  0.659 acc_train: 0.704 acc_test: 0.7\n",
      "epoch:  168 loss:  0.564 acc_train: 0.711 acc_test: 0.659\n",
      "epoch:  169 loss:  0.611 acc_train: 0.651 acc_test: 0.668\n",
      "epoch:  170 loss:  0.491 acc_train: 0.708 acc_test: 0.659\n",
      "epoch:  171 loss:  0.392 acc_train: 0.704 acc_test: 0.691\n",
      "epoch:  172 loss:  0.693 acc_train: 0.701 acc_test: 0.677\n",
      "epoch:  173 loss:  0.482 acc_train: 0.702 acc_test: 0.704\n",
      "epoch:  174 loss:  0.585 acc_train: 0.699 acc_test: 0.686\n",
      "epoch:  175 loss:  0.715 acc_train: 0.629 acc_test: 0.668\n",
      "epoch:  176 loss:  0.574 acc_train: 0.704 acc_test: 0.7\n",
      "epoch:  177 loss:  0.625 acc_train: 0.702 acc_test: 0.682\n",
      "epoch:  178 loss:  0.475 acc_train: 0.71 acc_test: 0.704\n",
      "epoch:  179 loss:  0.494 acc_train: 0.722 acc_test: 0.668\n",
      "epoch:  180 loss:  0.95 acc_train: 0.702 acc_test: 0.695\n",
      "epoch:  181 loss:  0.543 acc_train: 0.704 acc_test: 0.717\n",
      "epoch:  182 loss:  0.682 acc_train: 0.705 acc_test: 0.717\n",
      "epoch:  183 loss:  0.507 acc_train: 0.704 acc_test: 0.704\n",
      "epoch:  184 loss:  0.491 acc_train: 0.705 acc_test: 0.691\n",
      "epoch:  185 loss:  1.017 acc_train: 0.696 acc_test: 0.677\n",
      "epoch:  186 loss:  0.613 acc_train: 0.719 acc_test: 0.682\n",
      "epoch:  187 loss:  0.774 acc_train: 0.705 acc_test: 0.668\n",
      "epoch:  188 loss:  0.615 acc_train: 0.72 acc_test: 0.673\n",
      "epoch:  189 loss:  0.635 acc_train: 0.71 acc_test: 0.713\n",
      "epoch:  190 loss:  0.628 acc_train: 0.71 acc_test: 0.709\n",
      "epoch:  191 loss:  0.627 acc_train: 0.711 acc_test: 0.668\n",
      "epoch:  192 loss:  0.463 acc_train: 0.696 acc_test: 0.686\n",
      "epoch:  193 loss:  0.9 acc_train: 0.677 acc_test: 0.686\n",
      "epoch:  194 loss:  0.542 acc_train: 0.699 acc_test: 0.7\n",
      "epoch:  195 loss:  1.169 acc_train: 0.705 acc_test: 0.704\n",
      "epoch:  196 loss:  0.615 acc_train: 0.707 acc_test: 0.713\n",
      "epoch:  197 loss:  0.468 acc_train: 0.704 acc_test: 0.682\n",
      "epoch:  198 loss:  0.502 acc_train: 0.707 acc_test: 0.704\n",
      "epoch:  199 loss:  0.427 acc_train: 0.677 acc_test: 0.686\n",
      "epoch:  200 loss:  0.543 acc_train: 0.702 acc_test: 0.704\n",
      "epoch:  201 loss:  0.397 acc_train: 0.702 acc_test: 0.709\n",
      "epoch:  202 loss:  0.571 acc_train: 0.71 acc_test: 0.713\n",
      "epoch:  203 loss:  0.581 acc_train: 0.708 acc_test: 0.7\n",
      "epoch:  204 loss:  0.611 acc_train: 0.698 acc_test: 0.695\n",
      "epoch:  205 loss:  0.497 acc_train: 0.705 acc_test: 0.686\n",
      "epoch:  206 loss:  0.897 acc_train: 0.668 acc_test: 0.686\n",
      "epoch:  207 loss:  0.617 acc_train: 0.704 acc_test: 0.713\n",
      "epoch:  208 loss:  0.559 acc_train: 0.707 acc_test: 0.717\n",
      "epoch:  209 loss:  0.407 acc_train: 0.707 acc_test: 0.713\n",
      "epoch:  210 loss:  0.631 acc_train: 0.707 acc_test: 0.704\n",
      "epoch:  211 loss:  0.423 acc_train: 0.707 acc_test: 0.704\n",
      "epoch:  212 loss:  0.458 acc_train: 0.708 acc_test: 0.722\n",
      "epoch:  213 loss:  0.656 acc_train: 0.708 acc_test: 0.717\n",
      "epoch:  214 loss:  0.491 acc_train: 0.71 acc_test: 0.713\n",
      "epoch:  215 loss:  0.364 acc_train: 0.722 acc_test: 0.691\n",
      "epoch:  216 loss:  0.442 acc_train: 0.711 acc_test: 0.717\n",
      "epoch:  217 loss:  0.438 acc_train: 0.702 acc_test: 0.704\n",
      "epoch:  218 loss:  0.416 acc_train: 0.728 acc_test: 0.682\n",
      "epoch:  219 loss:  0.644 acc_train: 0.707 acc_test: 0.704\n",
      "epoch:  220 loss:  0.638 acc_train: 0.704 acc_test: 0.704\n",
      "epoch:  221 loss:  0.589 acc_train: 0.707 acc_test: 0.7\n",
      "epoch:  222 loss:  0.517 acc_train: 0.711 acc_test: 0.713\n",
      "epoch:  223 loss:  0.575 acc_train: 0.722 acc_test: 0.682\n",
      "epoch:  224 loss:  0.483 acc_train: 0.666 acc_test: 0.686\n",
      "epoch:  225 loss:  0.512 acc_train: 0.726 acc_test: 0.682\n",
      "epoch:  226 loss:  0.626 acc_train: 0.72 acc_test: 0.691\n",
      "epoch:  227 loss:  0.657 acc_train: 0.716 acc_test: 0.726\n",
      "epoch:  228 loss:  0.615 acc_train: 0.708 acc_test: 0.713\n",
      "epoch:  229 loss:  0.511 acc_train: 0.707 acc_test: 0.691\n",
      "epoch:  230 loss:  0.439 acc_train: 0.71 acc_test: 0.717\n",
      "epoch:  231 loss:  0.474 acc_train: 0.719 acc_test: 0.691\n",
      "epoch:  232 loss:  0.55 acc_train: 0.683 acc_test: 0.695\n",
      "epoch:  233 loss:  0.582 acc_train: 0.617 acc_test: 0.637\n",
      "epoch:  234 loss:  0.625 acc_train: 0.728 acc_test: 0.682\n",
      "epoch:  235 loss:  0.858 acc_train: 0.705 acc_test: 0.704\n",
      "epoch:  236 loss:  0.482 acc_train: 0.707 acc_test: 0.7\n",
      "epoch:  237 loss:  0.497 acc_train: 0.716 acc_test: 0.7\n",
      "epoch:  238 loss:  0.366 acc_train: 0.701 acc_test: 0.704\n",
      "epoch:  239 loss:  0.752 acc_train: 0.705 acc_test: 0.704\n",
      "epoch:  240 loss:  0.655 acc_train: 0.716 acc_test: 0.713\n",
      "epoch:  241 loss:  0.466 acc_train: 0.707 acc_test: 0.713\n",
      "epoch:  242 loss:  0.396 acc_train: 0.702 acc_test: 0.7\n",
      "epoch:  243 loss:  0.882 acc_train: 0.711 acc_test: 0.713\n",
      "epoch:  244 loss:  0.65 acc_train: 0.614 acc_test: 0.619\n",
      "epoch:  245 loss:  0.59 acc_train: 0.713 acc_test: 0.668\n",
      "epoch:  246 loss:  0.442 acc_train: 0.726 acc_test: 0.717\n",
      "epoch:  247 loss:  0.527 acc_train: 0.711 acc_test: 0.722\n",
      "epoch:  248 loss:  0.32 acc_train: 0.711 acc_test: 0.7\n",
      "epoch:  249 loss:  0.564 acc_train: 0.725 acc_test: 0.709\n",
      "epoch:  250 loss:  0.457 acc_train: 0.725 acc_test: 0.7\n",
      "epoch:  251 loss:  0.711 acc_train: 0.705 acc_test: 0.704\n",
      "epoch:  252 loss:  0.597 acc_train: 0.722 acc_test: 0.731\n",
      "epoch:  253 loss:  0.507 acc_train: 0.737 acc_test: 0.717\n",
      "epoch:  254 loss:  0.405 acc_train: 0.723 acc_test: 0.704\n",
      "epoch:  255 loss:  0.689 acc_train: 0.728 acc_test: 0.713\n",
      "epoch:  256 loss:  0.439 acc_train: 0.716 acc_test: 0.713\n",
      "epoch:  257 loss:  0.688 acc_train: 0.729 acc_test: 0.704\n",
      "epoch:  258 loss:  0.455 acc_train: 0.713 acc_test: 0.722\n",
      "epoch:  259 loss:  0.701 acc_train: 0.731 acc_test: 0.7\n",
      "epoch:  260 loss:  0.502 acc_train: 0.717 acc_test: 0.722\n",
      "epoch:  261 loss:  0.804 acc_train: 0.714 acc_test: 0.704\n",
      "epoch:  262 loss:  0.479 acc_train: 0.696 acc_test: 0.704\n",
      "epoch:  263 loss:  0.467 acc_train: 0.696 acc_test: 0.704\n",
      "epoch:  264 loss:  0.724 acc_train: 0.717 acc_test: 0.704\n",
      "epoch:  265 loss:  0.496 acc_train: 0.722 acc_test: 0.717\n",
      "epoch:  266 loss:  0.439 acc_train: 0.728 acc_test: 0.717\n",
      "epoch:  267 loss:  0.43 acc_train: 0.71 acc_test: 0.704\n",
      "epoch:  268 loss:  0.59 acc_train: 0.738 acc_test: 0.731\n",
      "epoch:  269 loss:  0.738 acc_train: 0.726 acc_test: 0.682\n",
      "epoch:  270 loss:  0.697 acc_train: 0.704 acc_test: 0.709\n",
      "epoch:  271 loss:  0.549 acc_train: 0.731 acc_test: 0.717\n",
      "epoch:  272 loss:  0.655 acc_train: 0.731 acc_test: 0.717\n",
      "epoch:  273 loss:  0.613 acc_train: 0.726 acc_test: 0.709\n",
      "epoch:  274 loss:  0.791 acc_train: 0.722 acc_test: 0.704\n",
      "epoch:  275 loss:  0.423 acc_train: 0.725 acc_test: 0.74\n",
      "epoch:  276 loss:  0.582 acc_train: 0.735 acc_test: 0.717\n",
      "epoch:  277 loss:  0.495 acc_train: 0.695 acc_test: 0.704\n",
      "epoch:  278 loss:  0.498 acc_train: 0.719 acc_test: 0.731\n",
      "epoch:  279 loss:  0.596 acc_train: 0.615 acc_test: 0.619\n",
      "epoch:  280 loss:  0.428 acc_train: 0.731 acc_test: 0.713\n",
      "epoch:  281 loss:  0.554 acc_train: 0.738 acc_test: 0.735\n",
      "epoch:  282 loss:  0.434 acc_train: 0.74 acc_test: 0.744\n",
      "epoch:  283 loss:  0.545 acc_train: 0.737 acc_test: 0.709\n",
      "epoch:  284 loss:  0.626 acc_train: 0.63 acc_test: 0.632\n",
      "epoch:  285 loss:  0.636 acc_train: 0.729 acc_test: 0.731\n",
      "epoch:  286 loss:  0.571 acc_train: 0.725 acc_test: 0.731\n",
      "epoch:  287 loss:  0.59 acc_train: 0.729 acc_test: 0.709\n",
      "epoch:  288 loss:  0.65 acc_train: 0.641 acc_test: 0.65\n",
      "epoch:  289 loss:  0.508 acc_train: 0.738 acc_test: 0.749\n",
      "epoch:  290 loss:  0.648 acc_train: 0.711 acc_test: 0.726\n",
      "epoch:  291 loss:  0.502 acc_train: 0.735 acc_test: 0.717\n",
      "epoch:  292 loss:  0.635 acc_train: 0.743 acc_test: 0.709\n",
      "epoch:  293 loss:  0.585 acc_train: 0.735 acc_test: 0.717\n",
      "epoch:  294 loss:  0.471 acc_train: 0.723 acc_test: 0.709\n",
      "epoch:  295 loss:  0.395 acc_train: 0.717 acc_test: 0.722\n",
      "epoch:  296 loss:  0.557 acc_train: 0.735 acc_test: 0.717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  297 loss:  0.434 acc_train: 0.726 acc_test: 0.726\n",
      "epoch:  298 loss:  0.721 acc_train: 0.725 acc_test: 0.731\n",
      "epoch:  299 loss:  0.582 acc_train: 0.729 acc_test: 0.726\n",
      "epoch:  300 loss:  0.473 acc_train: 0.75 acc_test: 0.771\n",
      "epoch:  301 loss:  0.492 acc_train: 0.735 acc_test: 0.735\n",
      "epoch:  302 loss:  0.407 acc_train: 0.723 acc_test: 0.704\n",
      "epoch:  303 loss:  0.59 acc_train: 0.719 acc_test: 0.717\n",
      "epoch:  304 loss:  0.635 acc_train: 0.725 acc_test: 0.704\n",
      "epoch:  305 loss:  0.361 acc_train: 0.74 acc_test: 0.744\n",
      "epoch:  306 loss:  0.587 acc_train: 0.725 acc_test: 0.731\n",
      "epoch:  307 loss:  0.541 acc_train: 0.711 acc_test: 0.713\n",
      "epoch:  308 loss:  0.571 acc_train: 0.732 acc_test: 0.735\n",
      "epoch:  309 loss:  0.633 acc_train: 0.737 acc_test: 0.744\n",
      "epoch:  310 loss:  0.608 acc_train: 0.729 acc_test: 0.726\n",
      "epoch:  311 loss:  0.494 acc_train: 0.717 acc_test: 0.717\n",
      "epoch:  312 loss:  0.521 acc_train: 0.719 acc_test: 0.709\n",
      "epoch:  313 loss:  0.509 acc_train: 0.741 acc_test: 0.753\n",
      "epoch:  314 loss:  0.385 acc_train: 0.737 acc_test: 0.735\n",
      "epoch:  315 loss:  0.497 acc_train: 0.731 acc_test: 0.731\n",
      "epoch:  316 loss:  0.472 acc_train: 0.716 acc_test: 0.709\n",
      "epoch:  317 loss:  0.69 acc_train: 0.74 acc_test: 0.762\n",
      "epoch:  318 loss:  0.733 acc_train: 0.729 acc_test: 0.735\n",
      "epoch:  319 loss:  0.437 acc_train: 0.734 acc_test: 0.74\n",
      "epoch:  320 loss:  0.575 acc_train: 0.693 acc_test: 0.709\n",
      "epoch:  321 loss:  0.416 acc_train: 0.749 acc_test: 0.753\n",
      "epoch:  322 loss:  0.513 acc_train: 0.747 acc_test: 0.744\n",
      "epoch:  323 loss:  0.57 acc_train: 0.726 acc_test: 0.722\n",
      "epoch:  324 loss:  0.654 acc_train: 0.714 acc_test: 0.722\n",
      "epoch:  325 loss:  0.528 acc_train: 0.735 acc_test: 0.722\n",
      "epoch:  326 loss:  0.551 acc_train: 0.734 acc_test: 0.735\n",
      "epoch:  327 loss:  0.421 acc_train: 0.737 acc_test: 0.735\n",
      "epoch:  328 loss:  0.574 acc_train: 0.747 acc_test: 0.749\n",
      "epoch:  329 loss:  0.609 acc_train: 0.705 acc_test: 0.713\n",
      "epoch:  330 loss:  0.453 acc_train: 0.744 acc_test: 0.749\n",
      "epoch:  331 loss:  0.4 acc_train: 0.704 acc_test: 0.717\n",
      "epoch:  332 loss:  0.539 acc_train: 0.726 acc_test: 0.717\n",
      "epoch:  333 loss:  0.471 acc_train: 0.74 acc_test: 0.74\n",
      "epoch:  334 loss:  0.451 acc_train: 0.746 acc_test: 0.744\n",
      "epoch:  335 loss:  0.523 acc_train: 0.743 acc_test: 0.753\n",
      "epoch:  336 loss:  0.454 acc_train: 0.738 acc_test: 0.74\n",
      "epoch:  337 loss:  0.405 acc_train: 0.751 acc_test: 0.762\n",
      "epoch:  338 loss:  0.747 acc_train: 0.747 acc_test: 0.758\n",
      "epoch:  339 loss:  0.325 acc_train: 0.741 acc_test: 0.726\n",
      "epoch:  340 loss:  0.455 acc_train: 0.753 acc_test: 0.767\n",
      "epoch:  341 loss:  0.525 acc_train: 0.732 acc_test: 0.735\n",
      "epoch:  342 loss:  0.525 acc_train: 0.756 acc_test: 0.767\n",
      "epoch:  343 loss:  0.497 acc_train: 0.737 acc_test: 0.731\n",
      "epoch:  344 loss:  0.538 acc_train: 0.76 acc_test: 0.771\n",
      "epoch:  345 loss:  0.514 acc_train: 0.671 acc_test: 0.704\n",
      "epoch:  346 loss:  0.425 acc_train: 0.737 acc_test: 0.74\n",
      "epoch:  347 loss:  0.603 acc_train: 0.741 acc_test: 0.758\n",
      "epoch:  348 loss:  0.424 acc_train: 0.737 acc_test: 0.74\n",
      "epoch:  349 loss:  0.546 acc_train: 0.746 acc_test: 0.749\n",
      "epoch:  350 loss:  0.603 acc_train: 0.751 acc_test: 0.753\n",
      "epoch:  351 loss:  0.755 acc_train: 0.741 acc_test: 0.731\n",
      "epoch:  352 loss:  0.453 acc_train: 0.749 acc_test: 0.758\n",
      "epoch:  353 loss:  0.427 acc_train: 0.749 acc_test: 0.749\n",
      "epoch:  354 loss:  0.47 acc_train: 0.731 acc_test: 0.735\n",
      "epoch:  355 loss:  0.418 acc_train: 0.692 acc_test: 0.713\n",
      "epoch:  356 loss:  0.445 acc_train: 0.729 acc_test: 0.735\n",
      "epoch:  357 loss:  0.49 acc_train: 0.747 acc_test: 0.762\n",
      "epoch:  358 loss:  0.536 acc_train: 0.754 acc_test: 0.767\n",
      "epoch:  359 loss:  0.558 acc_train: 0.744 acc_test: 0.758\n",
      "epoch:  360 loss:  0.502 acc_train: 0.751 acc_test: 0.767\n",
      "epoch:  361 loss:  1.109 acc_train: 0.741 acc_test: 0.735\n",
      "epoch:  362 loss:  0.777 acc_train: 0.751 acc_test: 0.771\n",
      "epoch:  363 loss:  0.381 acc_train: 0.744 acc_test: 0.762\n",
      "epoch:  364 loss:  0.558 acc_train: 0.753 acc_test: 0.771\n",
      "epoch:  365 loss:  0.743 acc_train: 0.737 acc_test: 0.74\n",
      "epoch:  366 loss:  0.491 acc_train: 0.735 acc_test: 0.74\n",
      "epoch:  367 loss:  0.616 acc_train: 0.744 acc_test: 0.744\n",
      "epoch:  368 loss:  0.793 acc_train: 0.74 acc_test: 0.744\n",
      "epoch:  369 loss:  0.329 acc_train: 0.75 acc_test: 0.735\n",
      "epoch:  370 loss:  0.446 acc_train: 0.722 acc_test: 0.722\n",
      "epoch:  371 loss:  0.449 acc_train: 0.738 acc_test: 0.731\n",
      "epoch:  372 loss:  0.672 acc_train: 0.746 acc_test: 0.758\n",
      "epoch:  373 loss:  0.569 acc_train: 0.756 acc_test: 0.767\n",
      "epoch:  374 loss:  0.464 acc_train: 0.76 acc_test: 0.78\n",
      "epoch:  375 loss:  0.316 acc_train: 0.741 acc_test: 0.744\n",
      "epoch:  376 loss:  0.736 acc_train: 0.756 acc_test: 0.789\n",
      "epoch:  377 loss:  0.249 acc_train: 0.734 acc_test: 0.762\n",
      "epoch:  378 loss:  0.67 acc_train: 0.749 acc_test: 0.758\n",
      "epoch:  379 loss:  0.748 acc_train: 0.741 acc_test: 0.753\n",
      "epoch:  380 loss:  0.584 acc_train: 0.717 acc_test: 0.731\n",
      "epoch:  381 loss:  0.67 acc_train: 0.749 acc_test: 0.753\n",
      "epoch:  382 loss:  0.328 acc_train: 0.762 acc_test: 0.785\n",
      "epoch:  383 loss:  0.497 acc_train: 0.716 acc_test: 0.722\n",
      "epoch:  384 loss:  0.576 acc_train: 0.756 acc_test: 0.789\n",
      "epoch:  385 loss:  0.521 acc_train: 0.754 acc_test: 0.785\n",
      "epoch:  386 loss:  0.649 acc_train: 0.743 acc_test: 0.758\n",
      "epoch:  387 loss:  0.572 acc_train: 0.753 acc_test: 0.762\n",
      "epoch:  388 loss:  0.363 acc_train: 0.751 acc_test: 0.776\n",
      "epoch:  389 loss:  0.6 acc_train: 0.746 acc_test: 0.753\n",
      "epoch:  390 loss:  0.606 acc_train: 0.74 acc_test: 0.744\n",
      "epoch:  391 loss:  0.649 acc_train: 0.756 acc_test: 0.78\n",
      "epoch:  392 loss:  0.522 acc_train: 0.75 acc_test: 0.762\n",
      "epoch:  393 loss:  0.624 acc_train: 0.757 acc_test: 0.789\n",
      "epoch:  394 loss:  0.621 acc_train: 0.746 acc_test: 0.758\n",
      "epoch:  395 loss:  0.341 acc_train: 0.756 acc_test: 0.789\n",
      "epoch:  396 loss:  0.605 acc_train: 0.751 acc_test: 0.767\n",
      "epoch:  397 loss:  0.583 acc_train: 0.75 acc_test: 0.753\n",
      "epoch:  398 loss:  0.6 acc_train: 0.76 acc_test: 0.789\n",
      "epoch:  399 loss:  0.418 acc_train: 0.743 acc_test: 0.744\n",
      "epoch:  400 loss:  0.727 acc_train: 0.737 acc_test: 0.735\n",
      "epoch:  401 loss:  0.396 acc_train: 0.749 acc_test: 0.767\n",
      "epoch:  402 loss:  0.39 acc_train: 0.747 acc_test: 0.776\n",
      "epoch:  403 loss:  0.348 acc_train: 0.754 acc_test: 0.78\n",
      "epoch:  404 loss:  0.765 acc_train: 0.751 acc_test: 0.762\n",
      "epoch:  405 loss:  0.452 acc_train: 0.747 acc_test: 0.767\n",
      "epoch:  406 loss:  0.824 acc_train: 0.756 acc_test: 0.785\n",
      "epoch:  407 loss:  0.444 acc_train: 0.699 acc_test: 0.722\n",
      "epoch:  408 loss:  0.546 acc_train: 0.746 acc_test: 0.767\n",
      "epoch:  409 loss:  0.453 acc_train: 0.759 acc_test: 0.785\n",
      "epoch:  410 loss:  0.429 acc_train: 0.76 acc_test: 0.789\n",
      "epoch:  411 loss:  0.457 acc_train: 0.762 acc_test: 0.785\n",
      "epoch:  412 loss:  0.619 acc_train: 0.768 acc_test: 0.803\n",
      "epoch:  413 loss:  0.592 acc_train: 0.747 acc_test: 0.762\n",
      "epoch:  414 loss:  0.429 acc_train: 0.766 acc_test: 0.794\n",
      "epoch:  415 loss:  0.433 acc_train: 0.756 acc_test: 0.767\n",
      "epoch:  416 loss:  0.499 acc_train: 0.749 acc_test: 0.753\n",
      "epoch:  417 loss:  0.519 acc_train: 0.759 acc_test: 0.78\n",
      "epoch:  418 loss:  0.754 acc_train: 0.753 acc_test: 0.767\n",
      "epoch:  419 loss:  0.769 acc_train: 0.762 acc_test: 0.789\n",
      "epoch:  420 loss:  0.526 acc_train: 0.759 acc_test: 0.794\n",
      "epoch:  421 loss:  0.573 acc_train: 0.759 acc_test: 0.789\n",
      "epoch:  422 loss:  0.42 acc_train: 0.75 acc_test: 0.776\n",
      "epoch:  423 loss:  0.638 acc_train: 0.749 acc_test: 0.767\n",
      "epoch:  424 loss:  0.466 acc_train: 0.751 acc_test: 0.767\n",
      "epoch:  425 loss:  0.489 acc_train: 0.762 acc_test: 0.785\n",
      "epoch:  426 loss:  0.445 acc_train: 0.75 acc_test: 0.758\n",
      "epoch:  427 loss:  0.449 acc_train: 0.757 acc_test: 0.785\n",
      "epoch:  428 loss:  0.416 acc_train: 0.753 acc_test: 0.785\n",
      "epoch:  429 loss:  0.569 acc_train: 0.753 acc_test: 0.776\n",
      "epoch:  430 loss:  0.436 acc_train: 0.757 acc_test: 0.785\n",
      "epoch:  431 loss:  0.64 acc_train: 0.75 acc_test: 0.776\n",
      "epoch:  432 loss:  0.629 acc_train: 0.754 acc_test: 0.767\n",
      "epoch:  433 loss:  0.684 acc_train: 0.746 acc_test: 0.758\n",
      "epoch:  434 loss:  0.501 acc_train: 0.75 acc_test: 0.749\n",
      "epoch:  435 loss:  0.644 acc_train: 0.757 acc_test: 0.789\n",
      "epoch:  436 loss:  0.531 acc_train: 0.744 acc_test: 0.758\n",
      "epoch:  437 loss:  0.598 acc_train: 0.666 acc_test: 0.668\n",
      "epoch:  438 loss:  0.587 acc_train: 0.756 acc_test: 0.789\n",
      "epoch:  439 loss:  0.499 acc_train: 0.763 acc_test: 0.794\n",
      "epoch:  440 loss:  0.382 acc_train: 0.757 acc_test: 0.767\n",
      "epoch:  441 loss:  0.538 acc_train: 0.756 acc_test: 0.789\n",
      "epoch:  442 loss:  0.543 acc_train: 0.749 acc_test: 0.749\n",
      "epoch:  443 loss:  0.606 acc_train: 0.76 acc_test: 0.794\n",
      "epoch:  444 loss:  0.551 acc_train: 0.768 acc_test: 0.794\n",
      "epoch:  445 loss:  0.384 acc_train: 0.76 acc_test: 0.776\n",
      "epoch:  446 loss:  0.367 acc_train: 0.751 acc_test: 0.744\n",
      "epoch:  447 loss:  0.859 acc_train: 0.754 acc_test: 0.789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  448 loss:  0.813 acc_train: 0.753 acc_test: 0.744\n",
      "epoch:  449 loss:  0.51 acc_train: 0.76 acc_test: 0.794\n",
      "epoch:  450 loss:  0.469 acc_train: 0.757 acc_test: 0.789\n",
      "epoch:  451 loss:  0.802 acc_train: 0.765 acc_test: 0.803\n",
      "epoch:  452 loss:  0.431 acc_train: 0.766 acc_test: 0.794\n",
      "epoch:  453 loss:  0.676 acc_train: 0.747 acc_test: 0.749\n",
      "epoch:  454 loss:  0.424 acc_train: 0.669 acc_test: 0.677\n",
      "epoch:  455 loss:  0.522 acc_train: 0.762 acc_test: 0.794\n",
      "epoch:  456 loss:  0.603 acc_train: 0.747 acc_test: 0.762\n",
      "epoch:  457 loss:  0.469 acc_train: 0.754 acc_test: 0.762\n",
      "epoch:  458 loss:  0.556 acc_train: 0.756 acc_test: 0.78\n",
      "epoch:  459 loss:  0.449 acc_train: 0.766 acc_test: 0.753\n",
      "epoch:  460 loss:  0.543 acc_train: 0.763 acc_test: 0.789\n",
      "epoch:  461 loss:  0.384 acc_train: 0.74 acc_test: 0.753\n",
      "epoch:  462 loss:  0.47 acc_train: 0.765 acc_test: 0.767\n",
      "epoch:  463 loss:  0.393 acc_train: 0.763 acc_test: 0.794\n",
      "epoch:  464 loss:  0.467 acc_train: 0.756 acc_test: 0.794\n",
      "epoch:  465 loss:  0.534 acc_train: 0.756 acc_test: 0.785\n",
      "epoch:  466 loss:  0.372 acc_train: 0.757 acc_test: 0.762\n",
      "epoch:  467 loss:  0.486 acc_train: 0.714 acc_test: 0.735\n",
      "epoch:  468 loss:  0.469 acc_train: 0.737 acc_test: 0.735\n",
      "epoch:  469 loss:  0.523 acc_train: 0.762 acc_test: 0.735\n",
      "epoch:  470 loss:  0.719 acc_train: 0.714 acc_test: 0.726\n",
      "epoch:  471 loss:  0.486 acc_train: 0.759 acc_test: 0.789\n",
      "epoch:  472 loss:  0.439 acc_train: 0.759 acc_test: 0.794\n",
      "epoch:  473 loss:  0.685 acc_train: 0.759 acc_test: 0.753\n",
      "epoch:  474 loss:  0.462 acc_train: 0.763 acc_test: 0.74\n",
      "epoch:  475 loss:  0.733 acc_train: 0.76 acc_test: 0.794\n",
      "epoch:  476 loss:  0.391 acc_train: 0.759 acc_test: 0.794\n",
      "epoch:  477 loss:  0.433 acc_train: 0.768 acc_test: 0.762\n",
      "epoch:  478 loss:  0.606 acc_train: 0.766 acc_test: 0.798\n",
      "epoch:  479 loss:  0.469 acc_train: 0.765 acc_test: 0.758\n",
      "epoch:  480 loss:  0.488 acc_train: 0.768 acc_test: 0.798\n",
      "epoch:  481 loss:  0.543 acc_train: 0.754 acc_test: 0.78\n",
      "epoch:  482 loss:  0.598 acc_train: 0.76 acc_test: 0.753\n",
      "epoch:  483 loss:  0.571 acc_train: 0.693 acc_test: 0.695\n",
      "epoch:  484 loss:  0.636 acc_train: 0.775 acc_test: 0.785\n",
      "epoch:  485 loss:  0.498 acc_train: 0.735 acc_test: 0.74\n",
      "epoch:  486 loss:  0.811 acc_train: 0.772 acc_test: 0.803\n",
      "epoch:  487 loss:  0.709 acc_train: 0.759 acc_test: 0.794\n",
      "epoch:  488 loss:  0.68 acc_train: 0.762 acc_test: 0.749\n",
      "epoch:  489 loss:  0.415 acc_train: 0.763 acc_test: 0.798\n",
      "epoch:  490 loss:  0.389 acc_train: 0.762 acc_test: 0.753\n",
      "epoch:  491 loss:  0.419 acc_train: 0.76 acc_test: 0.789\n",
      "epoch:  492 loss:  0.737 acc_train: 0.762 acc_test: 0.803\n",
      "epoch:  493 loss:  0.602 acc_train: 0.734 acc_test: 0.744\n",
      "epoch:  494 loss:  0.459 acc_train: 0.763 acc_test: 0.794\n",
      "epoch:  495 loss:  0.526 acc_train: 0.769 acc_test: 0.807\n",
      "epoch:  496 loss:  0.496 acc_train: 0.746 acc_test: 0.771\n",
      "epoch:  497 loss:  0.619 acc_train: 0.763 acc_test: 0.794\n",
      "epoch:  498 loss:  0.306 acc_train: 0.757 acc_test: 0.78\n",
      "epoch:  499 loss:  0.513 acc_train: 0.76 acc_test: 0.794\n",
      "epoch:  500 loss:  0.612 acc_train: 0.766 acc_test: 0.794\n",
      "epoch:  501 loss:  0.404 acc_train: 0.766 acc_test: 0.74\n",
      "epoch:  502 loss:  0.429 acc_train: 0.746 acc_test: 0.771\n",
      "epoch:  503 loss:  0.738 acc_train: 0.76 acc_test: 0.794\n",
      "epoch:  504 loss:  0.554 acc_train: 0.769 acc_test: 0.798\n",
      "epoch:  505 loss:  0.629 acc_train: 0.771 acc_test: 0.771\n",
      "epoch:  506 loss:  0.678 acc_train: 0.786 acc_test: 0.789\n",
      "epoch:  507 loss:  0.514 acc_train: 0.763 acc_test: 0.794\n",
      "epoch:  508 loss:  0.479 acc_train: 0.756 acc_test: 0.794\n",
      "epoch:  509 loss:  0.512 acc_train: 0.765 acc_test: 0.794\n",
      "epoch:  510 loss:  0.6 acc_train: 0.757 acc_test: 0.785\n",
      "epoch:  511 loss:  0.454 acc_train: 0.768 acc_test: 0.794\n",
      "epoch:  512 loss:  0.473 acc_train: 0.759 acc_test: 0.794\n",
      "epoch:  513 loss:  0.422 acc_train: 0.763 acc_test: 0.789\n",
      "epoch:  514 loss:  0.343 acc_train: 0.747 acc_test: 0.78\n",
      "epoch:  515 loss:  0.455 acc_train: 0.759 acc_test: 0.753\n",
      "epoch:  516 loss:  0.673 acc_train: 0.759 acc_test: 0.798\n",
      "epoch:  517 loss:  0.459 acc_train: 0.762 acc_test: 0.74\n",
      "epoch:  518 loss:  0.528 acc_train: 0.769 acc_test: 0.798\n",
      "epoch:  519 loss:  0.475 acc_train: 0.765 acc_test: 0.789\n",
      "epoch:  520 loss:  0.347 acc_train: 0.774 acc_test: 0.798\n",
      "epoch:  521 loss:  0.675 acc_train: 0.762 acc_test: 0.789\n",
      "epoch:  522 loss:  0.949 acc_train: 0.757 acc_test: 0.74\n",
      "epoch:  523 loss:  0.492 acc_train: 0.775 acc_test: 0.785\n",
      "epoch:  524 loss:  0.489 acc_train: 0.753 acc_test: 0.776\n",
      "epoch:  525 loss:  0.468 acc_train: 0.71 acc_test: 0.717\n",
      "epoch:  526 loss:  0.618 acc_train: 0.766 acc_test: 0.789\n",
      "epoch:  527 loss:  0.423 acc_train: 0.757 acc_test: 0.794\n",
      "epoch:  528 loss:  0.368 acc_train: 0.738 acc_test: 0.749\n",
      "epoch:  529 loss:  0.418 acc_train: 0.74 acc_test: 0.753\n",
      "epoch:  530 loss:  0.351 acc_train: 0.757 acc_test: 0.789\n",
      "epoch:  531 loss:  0.646 acc_train: 0.766 acc_test: 0.753\n",
      "epoch:  532 loss:  0.5 acc_train: 0.772 acc_test: 0.803\n",
      "epoch:  533 loss:  0.309 acc_train: 0.769 acc_test: 0.807\n",
      "epoch:  534 loss:  0.492 acc_train: 0.76 acc_test: 0.789\n",
      "epoch:  535 loss:  0.61 acc_train: 0.772 acc_test: 0.803\n",
      "epoch:  536 loss:  0.692 acc_train: 0.766 acc_test: 0.789\n",
      "epoch:  537 loss:  0.603 acc_train: 0.774 acc_test: 0.789\n",
      "epoch:  538 loss:  0.327 acc_train: 0.772 acc_test: 0.78\n",
      "epoch:  539 loss:  0.361 acc_train: 0.765 acc_test: 0.794\n",
      "epoch:  540 loss:  0.356 acc_train: 0.757 acc_test: 0.78\n",
      "epoch:  541 loss:  0.297 acc_train: 0.728 acc_test: 0.74\n",
      "epoch:  542 loss:  0.617 acc_train: 0.762 acc_test: 0.794\n",
      "epoch:  543 loss:  0.642 acc_train: 0.751 acc_test: 0.771\n",
      "epoch:  544 loss:  0.776 acc_train: 0.769 acc_test: 0.78\n",
      "epoch:  545 loss:  0.676 acc_train: 0.765 acc_test: 0.758\n",
      "epoch:  546 loss:  0.563 acc_train: 0.781 acc_test: 0.767\n",
      "epoch:  547 loss:  0.804 acc_train: 0.768 acc_test: 0.758\n",
      "epoch:  548 loss:  0.501 acc_train: 0.772 acc_test: 0.803\n",
      "epoch:  549 loss:  0.583 acc_train: 0.763 acc_test: 0.794\n",
      "epoch:  550 loss:  0.72 acc_train: 0.775 acc_test: 0.798\n",
      "epoch:  551 loss:  0.345 acc_train: 0.775 acc_test: 0.794\n",
      "epoch:  552 loss:  0.601 acc_train: 0.763 acc_test: 0.78\n",
      "epoch:  553 loss:  0.502 acc_train: 0.775 acc_test: 0.767\n",
      "epoch:  554 loss:  0.826 acc_train: 0.741 acc_test: 0.758\n",
      "epoch:  555 loss:  0.47 acc_train: 0.762 acc_test: 0.78\n",
      "epoch:  556 loss:  0.457 acc_train: 0.735 acc_test: 0.749\n",
      "epoch:  557 loss:  0.375 acc_train: 0.753 acc_test: 0.785\n",
      "epoch:  558 loss:  0.438 acc_train: 0.735 acc_test: 0.753\n",
      "epoch:  559 loss:  0.511 acc_train: 0.757 acc_test: 0.789\n",
      "epoch:  560 loss:  0.503 acc_train: 0.771 acc_test: 0.803\n",
      "epoch:  561 loss:  0.705 acc_train: 0.75 acc_test: 0.767\n",
      "epoch:  562 loss:  0.598 acc_train: 0.778 acc_test: 0.794\n",
      "epoch:  563 loss:  0.352 acc_train: 0.763 acc_test: 0.776\n",
      "epoch:  564 loss:  0.303 acc_train: 0.766 acc_test: 0.789\n",
      "epoch:  565 loss:  0.492 acc_train: 0.762 acc_test: 0.771\n",
      "epoch:  566 loss:  0.351 acc_train: 0.795 acc_test: 0.798\n",
      "epoch:  567 loss:  0.702 acc_train: 0.769 acc_test: 0.798\n",
      "epoch:  568 loss:  0.441 acc_train: 0.771 acc_test: 0.794\n",
      "epoch:  569 loss:  0.291 acc_train: 0.771 acc_test: 0.794\n",
      "epoch:  570 loss:  0.462 acc_train: 0.769 acc_test: 0.794\n",
      "epoch:  571 loss:  0.566 acc_train: 0.78 acc_test: 0.803\n",
      "epoch:  572 loss:  0.436 acc_train: 0.795 acc_test: 0.803\n",
      "epoch:  573 loss:  0.355 acc_train: 0.769 acc_test: 0.785\n",
      "epoch:  574 loss:  0.433 acc_train: 0.768 acc_test: 0.794\n",
      "epoch:  575 loss:  0.535 acc_train: 0.763 acc_test: 0.758\n",
      "epoch:  576 loss:  0.732 acc_train: 0.769 acc_test: 0.798\n",
      "epoch:  577 loss:  0.604 acc_train: 0.754 acc_test: 0.794\n",
      "epoch:  578 loss:  0.601 acc_train: 0.789 acc_test: 0.794\n",
      "epoch:  579 loss:  0.481 acc_train: 0.778 acc_test: 0.789\n",
      "epoch:  580 loss:  0.354 acc_train: 0.781 acc_test: 0.789\n",
      "epoch:  581 loss:  0.577 acc_train: 0.777 acc_test: 0.794\n",
      "epoch:  582 loss:  0.572 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  583 loss:  0.524 acc_train: 0.769 acc_test: 0.803\n",
      "epoch:  584 loss:  0.327 acc_train: 0.766 acc_test: 0.794\n",
      "epoch:  585 loss:  0.742 acc_train: 0.792 acc_test: 0.794\n",
      "epoch:  586 loss:  0.511 acc_train: 0.76 acc_test: 0.789\n",
      "epoch:  587 loss:  0.725 acc_train: 0.768 acc_test: 0.789\n",
      "epoch:  588 loss:  0.433 acc_train: 0.781 acc_test: 0.789\n",
      "epoch:  589 loss:  0.534 acc_train: 0.768 acc_test: 0.771\n",
      "epoch:  590 loss:  0.563 acc_train: 0.777 acc_test: 0.789\n",
      "epoch:  591 loss:  0.428 acc_train: 0.772 acc_test: 0.771\n",
      "epoch:  592 loss:  0.374 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  593 loss:  0.388 acc_train: 0.753 acc_test: 0.789\n",
      "epoch:  594 loss:  0.377 acc_train: 0.76 acc_test: 0.794\n",
      "epoch:  595 loss:  0.363 acc_train: 0.766 acc_test: 0.753\n",
      "epoch:  596 loss:  0.547 acc_train: 0.777 acc_test: 0.794\n",
      "epoch:  597 loss:  0.555 acc_train: 0.772 acc_test: 0.789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  598 loss:  0.548 acc_train: 0.766 acc_test: 0.789\n",
      "epoch:  599 loss:  0.638 acc_train: 0.775 acc_test: 0.798\n",
      "epoch:  600 loss:  0.504 acc_train: 0.772 acc_test: 0.794\n",
      "epoch:  601 loss:  0.393 acc_train: 0.774 acc_test: 0.758\n",
      "epoch:  602 loss:  0.714 acc_train: 0.774 acc_test: 0.794\n",
      "epoch:  603 loss:  0.673 acc_train: 0.772 acc_test: 0.753\n",
      "epoch:  604 loss:  0.304 acc_train: 0.766 acc_test: 0.753\n",
      "epoch:  605 loss:  0.415 acc_train: 0.771 acc_test: 0.758\n",
      "epoch:  606 loss:  0.32 acc_train: 0.777 acc_test: 0.798\n",
      "epoch:  607 loss:  0.56 acc_train: 0.778 acc_test: 0.794\n",
      "epoch:  608 loss:  0.581 acc_train: 0.774 acc_test: 0.794\n",
      "epoch:  609 loss:  0.776 acc_train: 0.781 acc_test: 0.744\n",
      "epoch:  610 loss:  0.426 acc_train: 0.774 acc_test: 0.753\n",
      "epoch:  611 loss:  0.55 acc_train: 0.763 acc_test: 0.744\n",
      "epoch:  612 loss:  0.44 acc_train: 0.762 acc_test: 0.771\n",
      "epoch:  613 loss:  0.452 acc_train: 0.772 acc_test: 0.758\n",
      "epoch:  614 loss:  0.44 acc_train: 0.769 acc_test: 0.794\n",
      "epoch:  615 loss:  0.372 acc_train: 0.763 acc_test: 0.753\n",
      "epoch:  616 loss:  0.376 acc_train: 0.795 acc_test: 0.798\n",
      "epoch:  617 loss:  0.688 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  618 loss:  0.596 acc_train: 0.763 acc_test: 0.74\n",
      "epoch:  619 loss:  0.332 acc_train: 0.781 acc_test: 0.758\n",
      "epoch:  620 loss:  0.399 acc_train: 0.789 acc_test: 0.794\n",
      "epoch:  621 loss:  0.617 acc_train: 0.769 acc_test: 0.794\n",
      "epoch:  622 loss:  0.454 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  623 loss:  0.684 acc_train: 0.777 acc_test: 0.803\n",
      "epoch:  624 loss:  0.423 acc_train: 0.781 acc_test: 0.789\n",
      "epoch:  625 loss:  0.54 acc_train: 0.781 acc_test: 0.789\n",
      "epoch:  626 loss:  0.511 acc_train: 0.789 acc_test: 0.794\n",
      "epoch:  627 loss:  0.517 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  628 loss:  0.727 acc_train: 0.763 acc_test: 0.794\n",
      "epoch:  629 loss:  0.625 acc_train: 0.78 acc_test: 0.758\n",
      "epoch:  630 loss:  0.572 acc_train: 0.775 acc_test: 0.789\n",
      "epoch:  631 loss:  0.568 acc_train: 0.777 acc_test: 0.798\n",
      "epoch:  632 loss:  0.494 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  633 loss:  0.406 acc_train: 0.774 acc_test: 0.794\n",
      "epoch:  634 loss:  0.516 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  635 loss:  0.518 acc_train: 0.789 acc_test: 0.794\n",
      "epoch:  636 loss:  0.392 acc_train: 0.771 acc_test: 0.789\n",
      "epoch:  637 loss:  0.722 acc_train: 0.768 acc_test: 0.789\n",
      "epoch:  638 loss:  0.247 acc_train: 0.777 acc_test: 0.803\n",
      "epoch:  639 loss:  0.482 acc_train: 0.769 acc_test: 0.798\n",
      "epoch:  640 loss:  0.599 acc_train: 0.768 acc_test: 0.789\n",
      "epoch:  641 loss:  0.435 acc_train: 0.793 acc_test: 0.803\n",
      "epoch:  642 loss:  0.34 acc_train: 0.766 acc_test: 0.789\n",
      "epoch:  643 loss:  0.467 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  644 loss:  0.804 acc_train: 0.777 acc_test: 0.789\n",
      "epoch:  645 loss:  0.686 acc_train: 0.766 acc_test: 0.753\n",
      "epoch:  646 loss:  0.45 acc_train: 0.778 acc_test: 0.803\n",
      "epoch:  647 loss:  0.684 acc_train: 0.772 acc_test: 0.785\n",
      "epoch:  648 loss:  0.503 acc_train: 0.766 acc_test: 0.744\n",
      "epoch:  649 loss:  0.318 acc_train: 0.775 acc_test: 0.785\n",
      "epoch:  650 loss:  0.487 acc_train: 0.778 acc_test: 0.789\n",
      "epoch:  651 loss:  0.42 acc_train: 0.778 acc_test: 0.798\n",
      "epoch:  652 loss:  0.32 acc_train: 0.766 acc_test: 0.794\n",
      "epoch:  653 loss:  0.439 acc_train: 0.777 acc_test: 0.785\n",
      "epoch:  654 loss:  0.564 acc_train: 0.778 acc_test: 0.794\n",
      "epoch:  655 loss:  0.645 acc_train: 0.79 acc_test: 0.798\n",
      "epoch:  656 loss:  0.461 acc_train: 0.763 acc_test: 0.753\n",
      "epoch:  657 loss:  0.386 acc_train: 0.775 acc_test: 0.794\n",
      "epoch:  658 loss:  0.481 acc_train: 0.749 acc_test: 0.785\n",
      "epoch:  659 loss:  0.746 acc_train: 0.769 acc_test: 0.794\n",
      "epoch:  660 loss:  0.466 acc_train: 0.783 acc_test: 0.758\n",
      "epoch:  661 loss:  0.573 acc_train: 0.783 acc_test: 0.771\n",
      "epoch:  662 loss:  0.528 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  663 loss:  0.607 acc_train: 0.774 acc_test: 0.758\n",
      "epoch:  664 loss:  0.372 acc_train: 0.766 acc_test: 0.789\n",
      "epoch:  665 loss:  0.348 acc_train: 0.787 acc_test: 0.798\n",
      "epoch:  666 loss:  0.536 acc_train: 0.768 acc_test: 0.794\n",
      "epoch:  667 loss:  0.388 acc_train: 0.763 acc_test: 0.789\n",
      "epoch:  668 loss:  0.445 acc_train: 0.783 acc_test: 0.762\n",
      "epoch:  669 loss:  0.229 acc_train: 0.771 acc_test: 0.776\n",
      "epoch:  670 loss:  0.557 acc_train: 0.766 acc_test: 0.753\n",
      "epoch:  671 loss:  0.736 acc_train: 0.772 acc_test: 0.758\n",
      "epoch:  672 loss:  0.478 acc_train: 0.768 acc_test: 0.789\n",
      "epoch:  673 loss:  0.409 acc_train: 0.778 acc_test: 0.794\n",
      "epoch:  674 loss:  0.954 acc_train: 0.777 acc_test: 0.785\n",
      "epoch:  675 loss:  0.418 acc_train: 0.765 acc_test: 0.753\n",
      "epoch:  676 loss:  0.265 acc_train: 0.793 acc_test: 0.798\n",
      "epoch:  677 loss:  0.26 acc_train: 0.762 acc_test: 0.785\n",
      "epoch:  678 loss:  0.822 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  679 loss:  0.45 acc_train: 0.787 acc_test: 0.798\n",
      "epoch:  680 loss:  0.57 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  681 loss:  0.457 acc_train: 0.795 acc_test: 0.803\n",
      "epoch:  682 loss:  0.515 acc_train: 0.746 acc_test: 0.758\n",
      "epoch:  683 loss:  0.473 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  684 loss:  0.458 acc_train: 0.778 acc_test: 0.803\n",
      "epoch:  685 loss:  0.347 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  686 loss:  0.384 acc_train: 0.768 acc_test: 0.794\n",
      "epoch:  687 loss:  0.395 acc_train: 0.769 acc_test: 0.758\n",
      "epoch:  688 loss:  0.512 acc_train: 0.781 acc_test: 0.789\n",
      "epoch:  689 loss:  0.368 acc_train: 0.771 acc_test: 0.776\n",
      "epoch:  690 loss:  0.647 acc_train: 0.781 acc_test: 0.789\n",
      "epoch:  691 loss:  0.585 acc_train: 0.774 acc_test: 0.794\n",
      "epoch:  692 loss:  0.474 acc_train: 0.802 acc_test: 0.812\n",
      "epoch:  693 loss:  0.372 acc_train: 0.786 acc_test: 0.794\n",
      "epoch:  694 loss:  0.448 acc_train: 0.778 acc_test: 0.798\n",
      "epoch:  695 loss:  0.525 acc_train: 0.795 acc_test: 0.807\n",
      "epoch:  696 loss:  0.639 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  697 loss:  0.534 acc_train: 0.777 acc_test: 0.798\n",
      "epoch:  698 loss:  0.635 acc_train: 0.763 acc_test: 0.753\n",
      "epoch:  699 loss:  0.664 acc_train: 0.792 acc_test: 0.798\n",
      "epoch:  700 loss:  0.474 acc_train: 0.777 acc_test: 0.794\n",
      "epoch:  701 loss:  0.404 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  702 loss:  0.396 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  703 loss:  0.592 acc_train: 0.775 acc_test: 0.803\n",
      "epoch:  704 loss:  0.249 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  705 loss:  0.445 acc_train: 0.777 acc_test: 0.794\n",
      "epoch:  706 loss:  0.612 acc_train: 0.784 acc_test: 0.776\n",
      "epoch:  707 loss:  0.393 acc_train: 0.768 acc_test: 0.753\n",
      "epoch:  708 loss:  0.286 acc_train: 0.795 acc_test: 0.807\n",
      "epoch:  709 loss:  0.453 acc_train: 0.777 acc_test: 0.794\n",
      "epoch:  710 loss:  0.582 acc_train: 0.792 acc_test: 0.803\n",
      "epoch:  711 loss:  0.625 acc_train: 0.777 acc_test: 0.794\n",
      "epoch:  712 loss:  0.293 acc_train: 0.774 acc_test: 0.794\n",
      "epoch:  713 loss:  0.559 acc_train: 0.778 acc_test: 0.753\n",
      "epoch:  714 loss:  0.365 acc_train: 0.777 acc_test: 0.785\n",
      "epoch:  715 loss:  0.637 acc_train: 0.775 acc_test: 0.789\n",
      "epoch:  716 loss:  0.401 acc_train: 0.772 acc_test: 0.771\n",
      "epoch:  717 loss:  0.537 acc_train: 0.777 acc_test: 0.789\n",
      "epoch:  718 loss:  0.569 acc_train: 0.777 acc_test: 0.794\n",
      "epoch:  719 loss:  0.421 acc_train: 0.777 acc_test: 0.785\n",
      "epoch:  720 loss:  0.727 acc_train: 0.768 acc_test: 0.744\n",
      "epoch:  721 loss:  0.452 acc_train: 0.781 acc_test: 0.789\n",
      "epoch:  722 loss:  0.521 acc_train: 0.765 acc_test: 0.789\n",
      "epoch:  723 loss:  0.516 acc_train: 0.765 acc_test: 0.78\n",
      "epoch:  724 loss:  0.608 acc_train: 0.766 acc_test: 0.785\n",
      "epoch:  725 loss:  0.535 acc_train: 0.783 acc_test: 0.785\n",
      "epoch:  726 loss:  0.451 acc_train: 0.787 acc_test: 0.794\n",
      "epoch:  727 loss:  0.333 acc_train: 0.778 acc_test: 0.794\n",
      "epoch:  728 loss:  1.3 acc_train: 0.781 acc_test: 0.753\n",
      "epoch:  729 loss:  0.486 acc_train: 0.793 acc_test: 0.812\n",
      "epoch:  730 loss:  0.255 acc_train: 0.777 acc_test: 0.803\n",
      "epoch:  731 loss:  0.372 acc_train: 0.78 acc_test: 0.767\n",
      "epoch:  732 loss:  0.416 acc_train: 0.78 acc_test: 0.767\n",
      "epoch:  733 loss:  0.361 acc_train: 0.784 acc_test: 0.762\n",
      "epoch:  734 loss:  0.421 acc_train: 0.766 acc_test: 0.789\n",
      "epoch:  735 loss:  0.634 acc_train: 0.763 acc_test: 0.753\n",
      "epoch:  736 loss:  0.522 acc_train: 0.771 acc_test: 0.785\n",
      "epoch:  737 loss:  0.385 acc_train: 0.778 acc_test: 0.789\n",
      "epoch:  738 loss:  0.385 acc_train: 0.792 acc_test: 0.803\n",
      "epoch:  739 loss:  0.441 acc_train: 0.775 acc_test: 0.785\n",
      "epoch:  740 loss:  0.237 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  741 loss:  0.314 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  742 loss:  0.396 acc_train: 0.801 acc_test: 0.812\n",
      "epoch:  743 loss:  0.66 acc_train: 0.778 acc_test: 0.785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  744 loss:  0.616 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  745 loss:  0.807 acc_train: 0.793 acc_test: 0.803\n",
      "epoch:  746 loss:  0.572 acc_train: 0.777 acc_test: 0.798\n",
      "epoch:  747 loss:  0.367 acc_train: 0.769 acc_test: 0.789\n",
      "epoch:  748 loss:  0.498 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  749 loss:  0.473 acc_train: 0.774 acc_test: 0.789\n",
      "epoch:  750 loss:  0.298 acc_train: 0.775 acc_test: 0.798\n",
      "epoch:  751 loss:  0.393 acc_train: 0.775 acc_test: 0.803\n",
      "epoch:  752 loss:  0.526 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  753 loss:  0.686 acc_train: 0.771 acc_test: 0.776\n",
      "epoch:  754 loss:  0.654 acc_train: 0.777 acc_test: 0.798\n",
      "epoch:  755 loss:  0.463 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  756 loss:  0.585 acc_train: 0.777 acc_test: 0.803\n",
      "epoch:  757 loss:  0.346 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  758 loss:  0.412 acc_train: 0.774 acc_test: 0.794\n",
      "epoch:  759 loss:  0.817 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  760 loss:  0.682 acc_train: 0.784 acc_test: 0.794\n",
      "epoch:  761 loss:  0.56 acc_train: 0.774 acc_test: 0.785\n",
      "epoch:  762 loss:  0.431 acc_train: 0.777 acc_test: 0.798\n",
      "epoch:  763 loss:  0.557 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  764 loss:  0.349 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  765 loss:  0.528 acc_train: 0.775 acc_test: 0.785\n",
      "epoch:  766 loss:  0.421 acc_train: 0.795 acc_test: 0.807\n",
      "epoch:  767 loss:  0.581 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  768 loss:  0.317 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  769 loss:  1.035 acc_train: 0.772 acc_test: 0.794\n",
      "epoch:  770 loss:  0.432 acc_train: 0.777 acc_test: 0.794\n",
      "epoch:  771 loss:  0.402 acc_train: 0.756 acc_test: 0.776\n",
      "epoch:  772 loss:  0.563 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  773 loss:  0.288 acc_train: 0.777 acc_test: 0.803\n",
      "epoch:  774 loss:  0.431 acc_train: 0.795 acc_test: 0.807\n",
      "epoch:  775 loss:  0.434 acc_train: 0.78 acc_test: 0.758\n",
      "epoch:  776 loss:  0.275 acc_train: 0.802 acc_test: 0.807\n",
      "epoch:  777 loss:  0.537 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  778 loss:  0.656 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  779 loss:  0.343 acc_train: 0.775 acc_test: 0.794\n",
      "epoch:  780 loss:  0.696 acc_train: 0.763 acc_test: 0.771\n",
      "epoch:  781 loss:  0.664 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  782 loss:  0.48 acc_train: 0.784 acc_test: 0.785\n",
      "epoch:  783 loss:  0.362 acc_train: 0.766 acc_test: 0.753\n",
      "epoch:  784 loss:  0.64 acc_train: 0.79 acc_test: 0.798\n",
      "epoch:  785 loss:  0.299 acc_train: 0.774 acc_test: 0.789\n",
      "epoch:  786 loss:  0.347 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  787 loss:  0.328 acc_train: 0.781 acc_test: 0.758\n",
      "epoch:  788 loss:  0.628 acc_train: 0.768 acc_test: 0.749\n",
      "epoch:  789 loss:  0.531 acc_train: 0.763 acc_test: 0.785\n",
      "epoch:  790 loss:  0.443 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  791 loss:  0.24 acc_train: 0.774 acc_test: 0.794\n",
      "epoch:  792 loss:  0.233 acc_train: 0.795 acc_test: 0.807\n",
      "epoch:  793 loss:  0.458 acc_train: 0.775 acc_test: 0.794\n",
      "epoch:  794 loss:  0.6 acc_train: 0.759 acc_test: 0.789\n",
      "epoch:  795 loss:  0.539 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  796 loss:  0.439 acc_train: 0.765 acc_test: 0.789\n",
      "epoch:  797 loss:  0.421 acc_train: 0.778 acc_test: 0.794\n",
      "epoch:  798 loss:  0.749 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  799 loss:  0.464 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  800 loss:  0.424 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  801 loss:  0.475 acc_train: 0.76 acc_test: 0.758\n",
      "epoch:  802 loss:  0.37 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  803 loss:  0.509 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  804 loss:  0.489 acc_train: 0.787 acc_test: 0.798\n",
      "epoch:  805 loss:  0.369 acc_train: 0.783 acc_test: 0.794\n",
      "epoch:  806 loss:  0.39 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  807 loss:  0.299 acc_train: 0.778 acc_test: 0.798\n",
      "epoch:  808 loss:  0.572 acc_train: 0.778 acc_test: 0.798\n",
      "epoch:  809 loss:  0.387 acc_train: 0.793 acc_test: 0.798\n",
      "epoch:  810 loss:  0.41 acc_train: 0.766 acc_test: 0.753\n",
      "epoch:  811 loss:  0.511 acc_train: 0.768 acc_test: 0.771\n",
      "epoch:  812 loss:  0.605 acc_train: 0.771 acc_test: 0.762\n",
      "epoch:  813 loss:  0.368 acc_train: 0.787 acc_test: 0.789\n",
      "epoch:  814 loss:  0.438 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  815 loss:  0.393 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  816 loss:  0.335 acc_train: 0.768 acc_test: 0.749\n",
      "epoch:  817 loss:  0.481 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  818 loss:  0.435 acc_train: 0.765 acc_test: 0.744\n",
      "epoch:  819 loss:  0.543 acc_train: 0.79 acc_test: 0.798\n",
      "epoch:  820 loss:  0.296 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  821 loss:  0.82 acc_train: 0.766 acc_test: 0.753\n",
      "epoch:  822 loss:  0.445 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  823 loss:  0.438 acc_train: 0.766 acc_test: 0.794\n",
      "epoch:  824 loss:  0.315 acc_train: 0.775 acc_test: 0.762\n",
      "epoch:  825 loss:  0.605 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  826 loss:  0.494 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  827 loss:  0.644 acc_train: 0.768 acc_test: 0.753\n",
      "epoch:  828 loss:  0.649 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  829 loss:  0.384 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  830 loss:  0.629 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  831 loss:  0.555 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  832 loss:  0.516 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  833 loss:  0.447 acc_train: 0.783 acc_test: 0.785\n",
      "epoch:  834 loss:  0.376 acc_train: 0.774 acc_test: 0.767\n",
      "epoch:  835 loss:  0.309 acc_train: 0.771 acc_test: 0.789\n",
      "epoch:  836 loss:  0.346 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  837 loss:  0.601 acc_train: 0.775 acc_test: 0.785\n",
      "epoch:  838 loss:  0.404 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  839 loss:  0.47 acc_train: 0.774 acc_test: 0.776\n",
      "epoch:  840 loss:  0.404 acc_train: 0.775 acc_test: 0.785\n",
      "epoch:  841 loss:  0.467 acc_train: 0.766 acc_test: 0.744\n",
      "epoch:  842 loss:  0.466 acc_train: 0.784 acc_test: 0.812\n",
      "epoch:  843 loss:  0.513 acc_train: 0.763 acc_test: 0.744\n",
      "epoch:  844 loss:  0.318 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  845 loss:  0.364 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  846 loss:  0.321 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  847 loss:  0.522 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  848 loss:  0.803 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  849 loss:  0.449 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  850 loss:  0.257 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  851 loss:  0.631 acc_train: 0.774 acc_test: 0.776\n",
      "epoch:  852 loss:  0.613 acc_train: 0.795 acc_test: 0.803\n",
      "epoch:  853 loss:  0.506 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  854 loss:  0.62 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  855 loss:  0.511 acc_train: 0.777 acc_test: 0.776\n",
      "epoch:  856 loss:  0.376 acc_train: 0.783 acc_test: 0.785\n",
      "epoch:  857 loss:  0.504 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  858 loss:  0.298 acc_train: 0.754 acc_test: 0.78\n",
      "epoch:  859 loss:  0.553 acc_train: 0.778 acc_test: 0.798\n",
      "epoch:  860 loss:  0.551 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  861 loss:  0.357 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  862 loss:  0.464 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  863 loss:  0.414 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  864 loss:  0.89 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  865 loss:  0.506 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  866 loss:  0.409 acc_train: 0.772 acc_test: 0.789\n",
      "epoch:  867 loss:  0.335 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  868 loss:  0.656 acc_train: 0.78 acc_test: 0.758\n",
      "epoch:  869 loss:  0.489 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  870 loss:  0.746 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  871 loss:  0.507 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  872 loss:  0.394 acc_train: 0.798 acc_test: 0.816\n",
      "epoch:  873 loss:  0.844 acc_train: 0.769 acc_test: 0.749\n",
      "epoch:  874 loss:  0.737 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  875 loss:  0.415 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  876 loss:  0.348 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  877 loss:  0.431 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  878 loss:  0.294 acc_train: 0.768 acc_test: 0.753\n",
      "epoch:  879 loss:  0.602 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  880 loss:  0.264 acc_train: 0.798 acc_test: 0.812\n",
      "epoch:  881 loss:  0.476 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  882 loss:  0.267 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  883 loss:  0.795 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  884 loss:  0.32 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  885 loss:  0.388 acc_train: 0.793 acc_test: 0.803\n",
      "epoch:  886 loss:  0.5 acc_train: 0.772 acc_test: 0.753\n",
      "epoch:  887 loss:  0.405 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  888 loss:  0.4 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  889 loss:  0.322 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  890 loss:  0.654 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  891 loss:  0.764 acc_train: 0.787 acc_test: 0.798\n",
      "epoch:  892 loss:  0.388 acc_train: 0.789 acc_test: 0.816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  893 loss:  0.477 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  894 loss:  0.408 acc_train: 0.768 acc_test: 0.749\n",
      "epoch:  895 loss:  0.516 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  896 loss:  0.383 acc_train: 0.774 acc_test: 0.767\n",
      "epoch:  897 loss:  0.508 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  898 loss:  0.313 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  899 loss:  0.642 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  900 loss:  0.755 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  901 loss:  0.516 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  902 loss:  0.522 acc_train: 0.789 acc_test: 0.816\n",
      "epoch:  903 loss:  0.754 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  904 loss:  0.405 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  905 loss:  0.41 acc_train: 0.783 acc_test: 0.794\n",
      "epoch:  906 loss:  0.381 acc_train: 0.762 acc_test: 0.744\n",
      "epoch:  907 loss:  0.428 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  908 loss:  0.511 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  909 loss:  0.423 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  910 loss:  0.316 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  911 loss:  0.389 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  912 loss:  0.502 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  913 loss:  0.313 acc_train: 0.775 acc_test: 0.789\n",
      "epoch:  914 loss:  0.412 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  915 loss:  0.667 acc_train: 0.766 acc_test: 0.753\n",
      "epoch:  916 loss:  0.572 acc_train: 0.772 acc_test: 0.771\n",
      "epoch:  917 loss:  0.45 acc_train: 0.774 acc_test: 0.78\n",
      "epoch:  918 loss:  0.283 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  919 loss:  0.331 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  920 loss:  0.443 acc_train: 0.746 acc_test: 0.776\n",
      "epoch:  921 loss:  0.401 acc_train: 0.78 acc_test: 0.771\n",
      "epoch:  922 loss:  0.68 acc_train: 0.79 acc_test: 0.798\n",
      "epoch:  923 loss:  0.439 acc_train: 0.78 acc_test: 0.785\n",
      "epoch:  924 loss:  0.567 acc_train: 0.781 acc_test: 0.78\n",
      "epoch:  925 loss:  0.475 acc_train: 0.768 acc_test: 0.749\n",
      "epoch:  926 loss:  0.583 acc_train: 0.775 acc_test: 0.776\n",
      "epoch:  927 loss:  0.597 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  928 loss:  0.574 acc_train: 0.789 acc_test: 0.816\n",
      "epoch:  929 loss:  0.746 acc_train: 0.763 acc_test: 0.785\n",
      "epoch:  930 loss:  0.977 acc_train: 0.79 acc_test: 0.798\n",
      "epoch:  931 loss:  0.377 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  932 loss:  0.639 acc_train: 0.793 acc_test: 0.812\n",
      "epoch:  933 loss:  0.604 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  934 loss:  0.587 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  935 loss:  0.446 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  936 loss:  0.241 acc_train: 0.775 acc_test: 0.803\n",
      "epoch:  937 loss:  0.536 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  938 loss:  0.684 acc_train: 0.796 acc_test: 0.816\n",
      "epoch:  939 loss:  0.581 acc_train: 0.784 acc_test: 0.789\n",
      "epoch:  940 loss:  0.397 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  941 loss:  0.511 acc_train: 0.751 acc_test: 0.776\n",
      "epoch:  942 loss:  0.776 acc_train: 0.79 acc_test: 0.798\n",
      "epoch:  943 loss:  0.702 acc_train: 0.771 acc_test: 0.758\n",
      "epoch:  944 loss:  0.567 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  945 loss:  1.125 acc_train: 0.781 acc_test: 0.771\n",
      "epoch:  946 loss:  0.462 acc_train: 0.793 acc_test: 0.816\n",
      "epoch:  947 loss:  0.575 acc_train: 0.778 acc_test: 0.798\n",
      "epoch:  948 loss:  0.328 acc_train: 0.783 acc_test: 0.785\n",
      "epoch:  949 loss:  0.344 acc_train: 0.769 acc_test: 0.753\n",
      "epoch:  950 loss:  0.517 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  951 loss:  0.505 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  952 loss:  0.568 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  953 loss:  0.396 acc_train: 0.771 acc_test: 0.789\n",
      "epoch:  954 loss:  0.421 acc_train: 0.802 acc_test: 0.816\n",
      "epoch:  955 loss:  0.405 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  956 loss:  0.483 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  957 loss:  0.485 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  958 loss:  0.679 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  959 loss:  0.443 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  960 loss:  0.418 acc_train: 0.768 acc_test: 0.744\n",
      "epoch:  961 loss:  0.556 acc_train: 0.795 acc_test: 0.816\n",
      "epoch:  962 loss:  0.548 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  963 loss:  0.7 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  964 loss:  0.828 acc_train: 0.765 acc_test: 0.785\n",
      "epoch:  965 loss:  0.487 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  966 loss:  0.354 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  967 loss:  0.654 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  968 loss:  0.358 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  969 loss:  0.576 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  970 loss:  0.504 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  971 loss:  0.535 acc_train: 0.781 acc_test: 0.794\n",
      "epoch:  972 loss:  0.389 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  973 loss:  0.5 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  974 loss:  0.36 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  975 loss:  0.32 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  976 loss:  0.504 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  977 loss:  0.349 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  978 loss:  0.491 acc_train: 0.778 acc_test: 0.794\n",
      "epoch:  979 loss:  0.238 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  980 loss:  0.478 acc_train: 0.78 acc_test: 0.803\n",
      "epoch:  981 loss:  0.499 acc_train: 0.768 acc_test: 0.753\n",
      "epoch:  982 loss:  0.408 acc_train: 0.783 acc_test: 0.789\n",
      "epoch:  983 loss:  0.507 acc_train: 0.781 acc_test: 0.78\n",
      "epoch:  984 loss:  0.597 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  985 loss:  0.349 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  986 loss:  0.331 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  987 loss:  0.38 acc_train: 0.768 acc_test: 0.744\n",
      "epoch:  988 loss:  0.398 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  989 loss:  0.364 acc_train: 0.762 acc_test: 0.753\n",
      "epoch:  990 loss:  0.451 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  991 loss:  0.305 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  992 loss:  0.371 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  993 loss:  0.707 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  994 loss:  0.477 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  995 loss:  0.714 acc_train: 0.766 acc_test: 0.744\n",
      "epoch:  996 loss:  0.538 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  997 loss:  0.541 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  998 loss:  0.341 acc_train: 0.772 acc_test: 0.753\n",
      "epoch:  999 loss:  0.583 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  1000 loss:  0.585 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1001 loss:  0.435 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1002 loss:  0.438 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  1003 loss:  0.419 acc_train: 0.778 acc_test: 0.807\n",
      "epoch:  1004 loss:  0.422 acc_train: 0.775 acc_test: 0.78\n",
      "epoch:  1005 loss:  0.413 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1006 loss:  0.497 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1007 loss:  0.542 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1008 loss:  0.367 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1009 loss:  0.452 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  1010 loss:  0.453 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1011 loss:  0.448 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1012 loss:  0.446 acc_train: 0.802 acc_test: 0.812\n",
      "epoch:  1013 loss:  0.464 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1014 loss:  0.43 acc_train: 0.79 acc_test: 0.798\n",
      "epoch:  1015 loss:  0.758 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1016 loss:  0.315 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  1017 loss:  0.469 acc_train: 0.771 acc_test: 0.753\n",
      "epoch:  1018 loss:  0.317 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1019 loss:  0.304 acc_train: 0.79 acc_test: 0.798\n",
      "epoch:  1020 loss:  0.334 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1021 loss:  0.634 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1022 loss:  0.435 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1023 loss:  0.338 acc_train: 0.775 acc_test: 0.789\n",
      "epoch:  1024 loss:  0.616 acc_train: 0.775 acc_test: 0.789\n",
      "epoch:  1025 loss:  0.424 acc_train: 0.792 acc_test: 0.803\n",
      "epoch:  1026 loss:  0.434 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1027 loss:  0.335 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1028 loss:  0.453 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1029 loss:  0.682 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1030 loss:  0.712 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1031 loss:  0.294 acc_train: 0.793 acc_test: 0.803\n",
      "epoch:  1032 loss:  0.544 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1033 loss:  0.463 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1034 loss:  0.42 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1035 loss:  0.625 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1036 loss:  0.568 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1037 loss:  0.45 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  1038 loss:  0.409 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1039 loss:  0.491 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1040 loss:  0.515 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1041 loss:  0.62 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1042 loss:  0.444 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1043 loss:  0.36 acc_train: 0.792 acc_test: 0.803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1044 loss:  0.312 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1045 loss:  0.498 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1046 loss:  0.58 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1047 loss:  0.394 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1048 loss:  0.557 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1049 loss:  0.613 acc_train: 0.799 acc_test: 0.807\n",
      "epoch:  1050 loss:  0.39 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1051 loss:  0.552 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1052 loss:  0.431 acc_train: 0.778 acc_test: 0.789\n",
      "epoch:  1053 loss:  0.416 acc_train: 0.799 acc_test: 0.807\n",
      "epoch:  1054 loss:  0.555 acc_train: 0.787 acc_test: 0.798\n",
      "epoch:  1055 loss:  0.375 acc_train: 0.787 acc_test: 0.798\n",
      "epoch:  1056 loss:  0.967 acc_train: 0.772 acc_test: 0.771\n",
      "epoch:  1057 loss:  0.465 acc_train: 0.793 acc_test: 0.812\n",
      "epoch:  1058 loss:  0.474 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1059 loss:  0.563 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1060 loss:  0.481 acc_train: 0.778 acc_test: 0.794\n",
      "epoch:  1061 loss:  0.35 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1062 loss:  0.347 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  1063 loss:  0.353 acc_train: 0.792 acc_test: 0.803\n",
      "epoch:  1064 loss:  0.527 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1065 loss:  0.614 acc_train: 0.79 acc_test: 0.789\n",
      "epoch:  1066 loss:  0.446 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1067 loss:  0.318 acc_train: 0.793 acc_test: 0.816\n",
      "epoch:  1068 loss:  0.426 acc_train: 0.778 acc_test: 0.794\n",
      "epoch:  1069 loss:  0.675 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1070 loss:  0.347 acc_train: 0.783 acc_test: 0.78\n",
      "epoch:  1071 loss:  0.307 acc_train: 0.775 acc_test: 0.776\n",
      "epoch:  1072 loss:  0.652 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1073 loss:  0.35 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1074 loss:  0.2 acc_train: 0.781 acc_test: 0.78\n",
      "epoch:  1075 loss:  0.505 acc_train: 0.771 acc_test: 0.753\n",
      "epoch:  1076 loss:  0.55 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1077 loss:  0.395 acc_train: 0.766 acc_test: 0.762\n",
      "epoch:  1078 loss:  0.422 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1079 loss:  0.573 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1080 loss:  0.462 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1081 loss:  0.393 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1082 loss:  0.307 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1083 loss:  0.345 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1084 loss:  0.381 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1085 loss:  0.371 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1086 loss:  0.372 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  1087 loss:  0.437 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1088 loss:  0.458 acc_train: 0.768 acc_test: 0.753\n",
      "epoch:  1089 loss:  0.563 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1090 loss:  0.422 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1091 loss:  0.591 acc_train: 0.801 acc_test: 0.807\n",
      "epoch:  1092 loss:  0.293 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1093 loss:  0.722 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1094 loss:  0.54 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  1095 loss:  0.37 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1096 loss:  0.434 acc_train: 0.777 acc_test: 0.776\n",
      "epoch:  1097 loss:  0.274 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1098 loss:  0.405 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1099 loss:  0.379 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  1100 loss:  0.302 acc_train: 0.777 acc_test: 0.785\n",
      "epoch:  1101 loss:  0.749 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1102 loss:  0.451 acc_train: 0.775 acc_test: 0.78\n",
      "epoch:  1103 loss:  0.544 acc_train: 0.778 acc_test: 0.789\n",
      "epoch:  1104 loss:  0.414 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1105 loss:  0.556 acc_train: 0.768 acc_test: 0.753\n",
      "epoch:  1106 loss:  0.406 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1107 loss:  0.653 acc_train: 0.771 acc_test: 0.753\n",
      "epoch:  1108 loss:  0.649 acc_train: 0.781 acc_test: 0.771\n",
      "epoch:  1109 loss:  0.524 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1110 loss:  0.309 acc_train: 0.772 acc_test: 0.762\n",
      "epoch:  1111 loss:  0.337 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1112 loss:  0.322 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1113 loss:  0.215 acc_train: 0.798 acc_test: 0.816\n",
      "epoch:  1114 loss:  0.418 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1115 loss:  0.266 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1116 loss:  0.39 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1117 loss:  0.453 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1118 loss:  0.288 acc_train: 0.763 acc_test: 0.744\n",
      "epoch:  1119 loss:  0.649 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1120 loss:  0.572 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  1121 loss:  0.484 acc_train: 0.784 acc_test: 0.812\n",
      "epoch:  1122 loss:  0.38 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1123 loss:  0.399 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1124 loss:  0.405 acc_train: 0.777 acc_test: 0.771\n",
      "epoch:  1125 loss:  0.551 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  1126 loss:  0.587 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  1127 loss:  0.709 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1128 loss:  0.443 acc_train: 0.778 acc_test: 0.794\n",
      "epoch:  1129 loss:  0.447 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1130 loss:  0.334 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  1131 loss:  0.258 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1132 loss:  0.467 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1133 loss:  0.367 acc_train: 0.793 acc_test: 0.812\n",
      "epoch:  1134 loss:  0.372 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  1135 loss:  0.735 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1136 loss:  0.65 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1137 loss:  0.709 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1138 loss:  0.401 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1139 loss:  0.374 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1140 loss:  0.737 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1141 loss:  0.791 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  1142 loss:  0.549 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1143 loss:  0.295 acc_train: 0.772 acc_test: 0.767\n",
      "epoch:  1144 loss:  0.546 acc_train: 0.792 acc_test: 0.803\n",
      "epoch:  1145 loss:  0.295 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1146 loss:  0.36 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1147 loss:  0.349 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1148 loss:  0.422 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1149 loss:  0.478 acc_train: 0.769 acc_test: 0.753\n",
      "epoch:  1150 loss:  0.518 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1151 loss:  0.417 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1152 loss:  0.369 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1153 loss:  0.366 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1154 loss:  0.396 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1155 loss:  0.508 acc_train: 0.774 acc_test: 0.767\n",
      "epoch:  1156 loss:  0.581 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1157 loss:  0.302 acc_train: 0.792 acc_test: 0.803\n",
      "epoch:  1158 loss:  0.395 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1159 loss:  0.773 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1160 loss:  0.443 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1161 loss:  0.63 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1162 loss:  0.336 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1163 loss:  0.426 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1164 loss:  0.655 acc_train: 0.766 acc_test: 0.753\n",
      "epoch:  1165 loss:  0.461 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1166 loss:  0.391 acc_train: 0.772 acc_test: 0.753\n",
      "epoch:  1167 loss:  0.462 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1168 loss:  0.414 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  1169 loss:  0.575 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1170 loss:  0.402 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1171 loss:  0.367 acc_train: 0.792 acc_test: 0.803\n",
      "epoch:  1172 loss:  0.343 acc_train: 0.796 acc_test: 0.812\n",
      "epoch:  1173 loss:  0.463 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1174 loss:  0.243 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1175 loss:  0.303 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1176 loss:  0.405 acc_train: 0.793 acc_test: 0.816\n",
      "epoch:  1177 loss:  0.457 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1178 loss:  0.432 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1179 loss:  0.329 acc_train: 0.798 acc_test: 0.803\n",
      "epoch:  1180 loss:  0.275 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1181 loss:  0.455 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  1182 loss:  0.287 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1183 loss:  0.329 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1184 loss:  0.339 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1185 loss:  0.285 acc_train: 0.784 acc_test: 0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1186 loss:  0.429 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1187 loss:  0.318 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1188 loss:  0.292 acc_train: 0.783 acc_test: 0.78\n",
      "epoch:  1189 loss:  0.724 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1190 loss:  0.35 acc_train: 0.769 acc_test: 0.749\n",
      "epoch:  1191 loss:  0.398 acc_train: 0.772 acc_test: 0.771\n",
      "epoch:  1192 loss:  0.502 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1193 loss:  0.334 acc_train: 0.777 acc_test: 0.785\n",
      "epoch:  1194 loss:  0.313 acc_train: 0.783 acc_test: 0.776\n",
      "epoch:  1195 loss:  0.386 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1196 loss:  0.308 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1197 loss:  0.455 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1198 loss:  0.592 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1199 loss:  0.337 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1200 loss:  1.01 acc_train: 0.775 acc_test: 0.771\n",
      "epoch:  1201 loss:  0.446 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  1202 loss:  0.459 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1203 loss:  1.056 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  1204 loss:  0.444 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1205 loss:  0.314 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1206 loss:  0.901 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1207 loss:  0.698 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1208 loss:  0.581 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1209 loss:  0.417 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1210 loss:  0.383 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1211 loss:  0.327 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1212 loss:  0.211 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1213 loss:  0.297 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1214 loss:  0.368 acc_train: 0.799 acc_test: 0.807\n",
      "epoch:  1215 loss:  0.436 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1216 loss:  0.616 acc_train: 0.801 acc_test: 0.807\n",
      "epoch:  1217 loss:  0.513 acc_train: 0.783 acc_test: 0.794\n",
      "epoch:  1218 loss:  0.383 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  1219 loss:  0.558 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1220 loss:  0.6 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1221 loss:  0.286 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1222 loss:  0.347 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1223 loss:  0.608 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1224 loss:  0.448 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1225 loss:  0.445 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1226 loss:  0.598 acc_train: 0.774 acc_test: 0.762\n",
      "epoch:  1227 loss:  0.458 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1228 loss:  0.71 acc_train: 0.777 acc_test: 0.785\n",
      "epoch:  1229 loss:  0.379 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1230 loss:  0.399 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1231 loss:  0.37 acc_train: 0.777 acc_test: 0.753\n",
      "epoch:  1232 loss:  0.46 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1233 loss:  0.251 acc_train: 0.793 acc_test: 0.798\n",
      "epoch:  1234 loss:  0.337 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1235 loss:  0.715 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1236 loss:  0.646 acc_train: 0.777 acc_test: 0.758\n",
      "epoch:  1237 loss:  0.443 acc_train: 0.799 acc_test: 0.807\n",
      "epoch:  1238 loss:  0.387 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1239 loss:  0.805 acc_train: 0.775 acc_test: 0.78\n",
      "epoch:  1240 loss:  0.472 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1241 loss:  0.6 acc_train: 0.774 acc_test: 0.749\n",
      "epoch:  1242 loss:  0.393 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1243 loss:  0.71 acc_train: 0.775 acc_test: 0.78\n",
      "epoch:  1244 loss:  0.572 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1245 loss:  0.434 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1246 loss:  0.598 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1247 loss:  0.587 acc_train: 0.792 acc_test: 0.803\n",
      "epoch:  1248 loss:  0.398 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1249 loss:  0.572 acc_train: 0.774 acc_test: 0.758\n",
      "epoch:  1250 loss:  0.344 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1251 loss:  0.651 acc_train: 0.784 acc_test: 0.812\n",
      "epoch:  1252 loss:  0.452 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1253 loss:  0.615 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1254 loss:  0.487 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1255 loss:  0.549 acc_train: 0.777 acc_test: 0.758\n",
      "epoch:  1256 loss:  0.789 acc_train: 0.799 acc_test: 0.807\n",
      "epoch:  1257 loss:  0.689 acc_train: 0.769 acc_test: 0.753\n",
      "epoch:  1258 loss:  0.432 acc_train: 0.774 acc_test: 0.776\n",
      "epoch:  1259 loss:  0.412 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1260 loss:  0.326 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1261 loss:  0.728 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1262 loss:  0.459 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1263 loss:  0.447 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1264 loss:  0.479 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1265 loss:  0.544 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1266 loss:  0.384 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1267 loss:  0.389 acc_train: 0.774 acc_test: 0.749\n",
      "epoch:  1268 loss:  0.559 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1269 loss:  0.283 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1270 loss:  0.884 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1271 loss:  0.492 acc_train: 0.781 acc_test: 0.794\n",
      "epoch:  1272 loss:  0.327 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1273 loss:  0.62 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1274 loss:  0.243 acc_train: 0.777 acc_test: 0.753\n",
      "epoch:  1275 loss:  0.569 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1276 loss:  0.855 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1277 loss:  1.057 acc_train: 0.789 acc_test: 0.816\n",
      "epoch:  1278 loss:  0.644 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1279 loss:  0.473 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1280 loss:  0.321 acc_train: 0.771 acc_test: 0.753\n",
      "epoch:  1281 loss:  0.429 acc_train: 0.768 acc_test: 0.753\n",
      "epoch:  1282 loss:  0.829 acc_train: 0.766 acc_test: 0.753\n",
      "epoch:  1283 loss:  0.528 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1284 loss:  0.295 acc_train: 0.781 acc_test: 0.78\n",
      "epoch:  1285 loss:  0.451 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  1286 loss:  0.406 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1287 loss:  0.652 acc_train: 0.795 acc_test: 0.816\n",
      "epoch:  1288 loss:  0.551 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  1289 loss:  0.311 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1290 loss:  0.761 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  1291 loss:  0.569 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1292 loss:  0.606 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1293 loss:  0.393 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1294 loss:  0.49 acc_train: 0.799 acc_test: 0.812\n",
      "epoch:  1295 loss:  0.33 acc_train: 0.769 acc_test: 0.744\n",
      "epoch:  1296 loss:  0.691 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1297 loss:  0.36 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1298 loss:  0.269 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1299 loss:  0.393 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1300 loss:  0.634 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  1301 loss:  0.28 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1302 loss:  0.512 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1303 loss:  0.303 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1304 loss:  0.619 acc_train: 0.772 acc_test: 0.758\n",
      "epoch:  1305 loss:  0.343 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1306 loss:  0.434 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1307 loss:  0.547 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1308 loss:  0.325 acc_train: 0.772 acc_test: 0.767\n",
      "epoch:  1309 loss:  0.512 acc_train: 0.789 acc_test: 0.816\n",
      "epoch:  1310 loss:  0.247 acc_train: 0.784 acc_test: 0.812\n",
      "epoch:  1311 loss:  0.551 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1312 loss:  0.707 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1313 loss:  0.433 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1314 loss:  0.511 acc_train: 0.798 acc_test: 0.816\n",
      "epoch:  1315 loss:  0.526 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1316 loss:  0.459 acc_train: 0.76 acc_test: 0.744\n",
      "epoch:  1317 loss:  0.494 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1318 loss:  0.446 acc_train: 0.787 acc_test: 0.798\n",
      "epoch:  1319 loss:  0.619 acc_train: 0.769 acc_test: 0.744\n",
      "epoch:  1320 loss:  0.375 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1321 loss:  0.245 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1322 loss:  0.608 acc_train: 0.763 acc_test: 0.749\n",
      "epoch:  1323 loss:  0.222 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1324 loss:  0.424 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1325 loss:  0.357 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  1326 loss:  0.922 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1327 loss:  0.441 acc_train: 0.787 acc_test: 0.803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1328 loss:  0.239 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1329 loss:  0.577 acc_train: 0.774 acc_test: 0.749\n",
      "epoch:  1330 loss:  0.77 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1331 loss:  0.237 acc_train: 0.772 acc_test: 0.758\n",
      "epoch:  1332 loss:  0.635 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1333 loss:  0.422 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1334 loss:  0.882 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1335 loss:  0.608 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1336 loss:  0.588 acc_train: 0.789 acc_test: 0.816\n",
      "epoch:  1337 loss:  0.368 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1338 loss:  0.639 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1339 loss:  0.501 acc_train: 0.793 acc_test: 0.816\n",
      "epoch:  1340 loss:  0.674 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1341 loss:  0.613 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  1342 loss:  0.336 acc_train: 0.78 acc_test: 0.767\n",
      "epoch:  1343 loss:  0.399 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1344 loss:  0.452 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1345 loss:  0.569 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1346 loss:  0.415 acc_train: 0.772 acc_test: 0.753\n",
      "epoch:  1347 loss:  0.481 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1348 loss:  0.337 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1349 loss:  0.251 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1350 loss:  0.701 acc_train: 0.786 acc_test: 0.789\n",
      "epoch:  1351 loss:  0.474 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1352 loss:  0.44 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1353 loss:  0.264 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1354 loss:  0.73 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1355 loss:  0.59 acc_train: 0.792 acc_test: 0.803\n",
      "epoch:  1356 loss:  0.636 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1357 loss:  0.313 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1358 loss:  0.538 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1359 loss:  0.542 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1360 loss:  0.814 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1361 loss:  0.292 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1362 loss:  0.347 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1363 loss:  0.32 acc_train: 0.772 acc_test: 0.762\n",
      "epoch:  1364 loss:  0.43 acc_train: 0.777 acc_test: 0.78\n",
      "epoch:  1365 loss:  0.44 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1366 loss:  0.2 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1367 loss:  0.338 acc_train: 0.793 acc_test: 0.816\n",
      "epoch:  1368 loss:  0.47 acc_train: 0.799 acc_test: 0.807\n",
      "epoch:  1369 loss:  0.333 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1370 loss:  0.315 acc_train: 0.771 acc_test: 0.744\n",
      "epoch:  1371 loss:  0.62 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1372 loss:  0.327 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  1373 loss:  0.559 acc_train: 0.799 acc_test: 0.807\n",
      "epoch:  1374 loss:  0.559 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1375 loss:  0.562 acc_train: 0.775 acc_test: 0.762\n",
      "epoch:  1376 loss:  0.532 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1377 loss:  0.875 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1378 loss:  0.389 acc_train: 0.775 acc_test: 0.776\n",
      "epoch:  1379 loss:  0.661 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1380 loss:  0.537 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  1381 loss:  0.464 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  1382 loss:  0.504 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1383 loss:  0.345 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1384 loss:  0.41 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1385 loss:  0.436 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1386 loss:  0.743 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1387 loss:  0.508 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1388 loss:  0.258 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1389 loss:  0.362 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1390 loss:  0.344 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1391 loss:  0.37 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1392 loss:  0.524 acc_train: 0.78 acc_test: 0.785\n",
      "epoch:  1393 loss:  0.163 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1394 loss:  0.587 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1395 loss:  0.321 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1396 loss:  0.707 acc_train: 0.775 acc_test: 0.776\n",
      "epoch:  1397 loss:  0.258 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  1398 loss:  0.462 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1399 loss:  0.51 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  1400 loss:  0.266 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1401 loss:  0.361 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1402 loss:  0.301 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  1403 loss:  0.504 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1404 loss:  0.716 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1405 loss:  0.63 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1406 loss:  0.533 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1407 loss:  0.28 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1408 loss:  0.587 acc_train: 0.783 acc_test: 0.789\n",
      "epoch:  1409 loss:  0.594 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1410 loss:  0.532 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  1411 loss:  0.399 acc_train: 0.777 acc_test: 0.749\n",
      "epoch:  1412 loss:  0.341 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  1413 loss:  0.345 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1414 loss:  0.49 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1415 loss:  0.383 acc_train: 0.774 acc_test: 0.758\n",
      "epoch:  1416 loss:  0.473 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1417 loss:  0.831 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  1418 loss:  0.575 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1419 loss:  0.419 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1420 loss:  0.492 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1421 loss:  0.439 acc_train: 0.771 acc_test: 0.744\n",
      "epoch:  1422 loss:  0.449 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1423 loss:  0.592 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1424 loss:  0.205 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1425 loss:  0.733 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1426 loss:  0.305 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1427 loss:  0.331 acc_train: 0.787 acc_test: 0.798\n",
      "epoch:  1428 loss:  0.728 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  1429 loss:  0.562 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  1430 loss:  0.645 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1431 loss:  0.545 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1432 loss:  0.415 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  1433 loss:  0.547 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1434 loss:  0.713 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1435 loss:  0.255 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  1436 loss:  0.539 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1437 loss:  0.174 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1438 loss:  0.646 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1439 loss:  0.45 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1440 loss:  0.559 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1441 loss:  0.455 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1442 loss:  0.651 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1443 loss:  0.955 acc_train: 0.787 acc_test: 0.798\n",
      "epoch:  1444 loss:  0.521 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  1445 loss:  0.394 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1446 loss:  0.384 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1447 loss:  0.456 acc_train: 0.798 acc_test: 0.812\n",
      "epoch:  1448 loss:  0.371 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1449 loss:  0.467 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1450 loss:  0.677 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  1451 loss:  0.456 acc_train: 0.802 acc_test: 0.807\n",
      "epoch:  1452 loss:  0.768 acc_train: 0.784 acc_test: 0.812\n",
      "epoch:  1453 loss:  0.349 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1454 loss:  0.286 acc_train: 0.792 acc_test: 0.803\n",
      "epoch:  1455 loss:  0.527 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1456 loss:  0.606 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1457 loss:  0.72 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1458 loss:  0.252 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1459 loss:  0.662 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  1460 loss:  0.488 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1461 loss:  0.554 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1462 loss:  0.558 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  1463 loss:  0.565 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1464 loss:  0.349 acc_train: 0.775 acc_test: 0.749\n",
      "epoch:  1465 loss:  1.164 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  1466 loss:  0.476 acc_train: 0.769 acc_test: 0.749\n",
      "epoch:  1467 loss:  0.648 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  1468 loss:  0.487 acc_train: 0.799 acc_test: 0.812\n",
      "epoch:  1469 loss:  0.555 acc_train: 0.786 acc_test: 0.803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1470 loss:  0.477 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1471 loss:  0.484 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1472 loss:  0.282 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1473 loss:  0.287 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1474 loss:  0.364 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1475 loss:  0.342 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1476 loss:  0.434 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1477 loss:  0.506 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  1478 loss:  0.353 acc_train: 0.798 acc_test: 0.812\n",
      "epoch:  1479 loss:  0.401 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1480 loss:  0.676 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  1481 loss:  0.219 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1482 loss:  0.459 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1483 loss:  0.313 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1484 loss:  0.654 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  1485 loss:  0.462 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1486 loss:  0.64 acc_train: 0.793 acc_test: 0.812\n",
      "epoch:  1487 loss:  0.451 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1488 loss:  0.349 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  1489 loss:  0.701 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1490 loss:  0.398 acc_train: 0.801 acc_test: 0.812\n",
      "epoch:  1491 loss:  0.534 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  1492 loss:  0.491 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1493 loss:  0.386 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  1494 loss:  0.363 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1495 loss:  0.585 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  1496 loss:  0.51 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1497 loss:  0.399 acc_train: 0.798 acc_test: 0.807\n",
      "epoch:  1498 loss:  0.436 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1499 loss:  0.227 acc_train: 0.799 acc_test: 0.807\n",
      "epoch:  1500 loss:  0.543 acc_train: 0.789 acc_test: 0.816\n",
      "epoch:  1501 loss:  0.683 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1502 loss:  0.358 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1503 loss:  0.615 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  1504 loss:  0.309 acc_train: 0.798 acc_test: 0.807\n",
      "epoch:  1505 loss:  0.226 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1506 loss:  0.987 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  1507 loss:  0.464 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  1508 loss:  0.492 acc_train: 0.775 acc_test: 0.771\n",
      "epoch:  1509 loss:  0.424 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1510 loss:  0.697 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1511 loss:  0.643 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1512 loss:  0.431 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  1513 loss:  0.447 acc_train: 0.778 acc_test: 0.78\n",
      "epoch:  1514 loss:  0.541 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1515 loss:  0.528 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1516 loss:  0.326 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1517 loss:  0.225 acc_train: 0.771 acc_test: 0.749\n",
      "epoch:  1518 loss:  0.293 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1519 loss:  0.474 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1520 loss:  0.374 acc_train: 0.771 acc_test: 0.758\n",
      "epoch:  1521 loss:  0.466 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1522 loss:  0.428 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1523 loss:  0.782 acc_train: 0.799 acc_test: 0.812\n",
      "epoch:  1524 loss:  0.271 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1525 loss:  0.37 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1526 loss:  0.511 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1527 loss:  0.479 acc_train: 0.769 acc_test: 0.771\n",
      "epoch:  1528 loss:  0.493 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1529 loss:  0.681 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1530 loss:  0.608 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1531 loss:  0.71 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  1532 loss:  0.306 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1533 loss:  0.412 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1534 loss:  0.43 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1535 loss:  0.517 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1536 loss:  0.402 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1537 loss:  0.496 acc_train: 0.78 acc_test: 0.785\n",
      "epoch:  1538 loss:  0.66 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1539 loss:  0.772 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1540 loss:  0.687 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1541 loss:  0.367 acc_train: 0.784 acc_test: 0.812\n",
      "epoch:  1542 loss:  0.538 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1543 loss:  0.361 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1544 loss:  0.803 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1545 loss:  0.267 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1546 loss:  0.771 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1547 loss:  0.472 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1548 loss:  0.214 acc_train: 0.775 acc_test: 0.771\n",
      "epoch:  1549 loss:  0.41 acc_train: 0.777 acc_test: 0.758\n",
      "epoch:  1550 loss:  0.498 acc_train: 0.777 acc_test: 0.776\n",
      "epoch:  1551 loss:  0.395 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1552 loss:  0.36 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1553 loss:  0.319 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1554 loss:  0.264 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  1555 loss:  0.347 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1556 loss:  0.397 acc_train: 0.775 acc_test: 0.776\n",
      "epoch:  1557 loss:  0.496 acc_train: 0.769 acc_test: 0.749\n",
      "epoch:  1558 loss:  0.514 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1559 loss:  0.274 acc_train: 0.787 acc_test: 0.789\n",
      "epoch:  1560 loss:  0.235 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1561 loss:  0.57 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  1562 loss:  0.496 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1563 loss:  0.321 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  1564 loss:  0.309 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1565 loss:  0.498 acc_train: 0.777 acc_test: 0.776\n",
      "epoch:  1566 loss:  0.823 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1567 loss:  0.363 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1568 loss:  0.409 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1569 loss:  0.25 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1570 loss:  0.594 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1571 loss:  0.608 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1572 loss:  0.801 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1573 loss:  0.328 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1574 loss:  0.197 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1575 loss:  0.447 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1576 loss:  0.406 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1577 loss:  0.388 acc_train: 0.789 acc_test: 0.816\n",
      "epoch:  1578 loss:  0.395 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1579 loss:  0.285 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1580 loss:  0.404 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  1581 loss:  0.775 acc_train: 0.774 acc_test: 0.753\n",
      "epoch:  1582 loss:  0.245 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1583 loss:  0.368 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1584 loss:  0.566 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1585 loss:  0.616 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1586 loss:  0.566 acc_train: 0.775 acc_test: 0.767\n",
      "epoch:  1587 loss:  0.597 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1588 loss:  0.425 acc_train: 0.781 acc_test: 0.78\n",
      "epoch:  1589 loss:  0.695 acc_train: 0.799 acc_test: 0.812\n",
      "epoch:  1590 loss:  0.46 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1591 loss:  0.48 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1592 loss:  0.625 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1593 loss:  0.612 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1594 loss:  0.473 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1595 loss:  0.367 acc_train: 0.777 acc_test: 0.78\n",
      "epoch:  1596 loss:  0.586 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1597 loss:  0.318 acc_train: 0.795 acc_test: 0.816\n",
      "epoch:  1598 loss:  0.392 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1599 loss:  0.343 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  1600 loss:  0.756 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1601 loss:  0.446 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1602 loss:  0.826 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1603 loss:  0.285 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1604 loss:  0.182 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1605 loss:  0.451 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1606 loss:  0.51 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1607 loss:  0.711 acc_train: 0.78 acc_test: 0.785\n",
      "epoch:  1608 loss:  0.363 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  1609 loss:  0.249 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1610 loss:  0.371 acc_train: 0.781 acc_test: 0.798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1611 loss:  0.398 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1612 loss:  0.511 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1613 loss:  0.624 acc_train: 0.795 acc_test: 0.794\n",
      "epoch:  1614 loss:  0.399 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1615 loss:  0.389 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1616 loss:  0.466 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1617 loss:  0.407 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1618 loss:  0.688 acc_train: 0.783 acc_test: 0.776\n",
      "epoch:  1619 loss:  0.331 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1620 loss:  0.506 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1621 loss:  0.614 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1622 loss:  0.388 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1623 loss:  0.376 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1624 loss:  0.131 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  1625 loss:  0.398 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1626 loss:  0.245 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  1627 loss:  0.268 acc_train: 0.796 acc_test: 0.812\n",
      "epoch:  1628 loss:  0.573 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1629 loss:  0.429 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1630 loss:  0.267 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1631 loss:  0.547 acc_train: 0.771 acc_test: 0.767\n",
      "epoch:  1632 loss:  0.343 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1633 loss:  0.387 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1634 loss:  0.325 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1635 loss:  0.276 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1636 loss:  0.356 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1637 loss:  0.579 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1638 loss:  0.587 acc_train: 0.774 acc_test: 0.762\n",
      "epoch:  1639 loss:  0.613 acc_train: 0.793 acc_test: 0.803\n",
      "epoch:  1640 loss:  0.353 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1641 loss:  0.582 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1642 loss:  0.179 acc_train: 0.774 acc_test: 0.753\n",
      "epoch:  1643 loss:  0.475 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  1644 loss:  0.231 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1645 loss:  0.481 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  1646 loss:  0.465 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1647 loss:  0.389 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1648 loss:  0.392 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1649 loss:  0.645 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1650 loss:  0.77 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1651 loss:  0.832 acc_train: 0.777 acc_test: 0.753\n",
      "epoch:  1652 loss:  0.538 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1653 loss:  0.271 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  1654 loss:  0.553 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1655 loss:  0.339 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1656 loss:  0.421 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1657 loss:  0.364 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1658 loss:  0.681 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1659 loss:  0.613 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  1660 loss:  0.429 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1661 loss:  0.299 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1662 loss:  0.439 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1663 loss:  0.671 acc_train: 0.789 acc_test: 0.816\n",
      "epoch:  1664 loss:  0.392 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1665 loss:  0.486 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1666 loss:  0.543 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1667 loss:  0.288 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1668 loss:  0.538 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1669 loss:  0.53 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1670 loss:  0.576 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  1671 loss:  0.605 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1672 loss:  0.389 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1673 loss:  0.52 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1674 loss:  0.337 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  1675 loss:  0.596 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  1676 loss:  0.767 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1677 loss:  0.402 acc_train: 0.775 acc_test: 0.776\n",
      "epoch:  1678 loss:  0.32 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1679 loss:  0.544 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1680 loss:  0.583 acc_train: 0.774 acc_test: 0.771\n",
      "epoch:  1681 loss:  0.27 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1682 loss:  0.199 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1683 loss:  0.441 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1684 loss:  0.303 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1685 loss:  0.479 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1686 loss:  0.321 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  1687 loss:  0.827 acc_train: 0.778 acc_test: 0.78\n",
      "epoch:  1688 loss:  0.512 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1689 loss:  0.755 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1690 loss:  0.133 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1691 loss:  0.556 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1692 loss:  0.982 acc_train: 0.774 acc_test: 0.753\n",
      "epoch:  1693 loss:  0.766 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1694 loss:  0.662 acc_train: 0.771 acc_test: 0.753\n",
      "epoch:  1695 loss:  0.515 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1696 loss:  0.817 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  1697 loss:  0.593 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1698 loss:  0.306 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1699 loss:  0.655 acc_train: 0.777 acc_test: 0.753\n",
      "epoch:  1700 loss:  0.337 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1701 loss:  0.399 acc_train: 0.793 acc_test: 0.816\n",
      "epoch:  1702 loss:  0.694 acc_train: 0.798 acc_test: 0.812\n",
      "epoch:  1703 loss:  0.623 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1704 loss:  0.416 acc_train: 0.777 acc_test: 0.767\n",
      "epoch:  1705 loss:  0.508 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1706 loss:  0.324 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1707 loss:  0.394 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1708 loss:  0.464 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1709 loss:  0.334 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1710 loss:  0.505 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  1711 loss:  0.769 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1712 loss:  0.534 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1713 loss:  0.494 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1714 loss:  0.468 acc_train: 0.777 acc_test: 0.776\n",
      "epoch:  1715 loss:  0.603 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1716 loss:  0.548 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1717 loss:  0.547 acc_train: 0.778 acc_test: 0.789\n",
      "epoch:  1718 loss:  0.282 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1719 loss:  0.518 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1720 loss:  0.437 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1721 loss:  0.63 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  1722 loss:  0.453 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  1723 loss:  0.341 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  1724 loss:  0.32 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1725 loss:  0.354 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1726 loss:  0.385 acc_train: 0.795 acc_test: 0.816\n",
      "epoch:  1727 loss:  0.299 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  1728 loss:  0.269 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  1729 loss:  0.406 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1730 loss:  0.537 acc_train: 0.781 acc_test: 0.785\n",
      "epoch:  1731 loss:  0.521 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1732 loss:  0.486 acc_train: 0.774 acc_test: 0.744\n",
      "epoch:  1733 loss:  0.549 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1734 loss:  0.343 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1735 loss:  0.649 acc_train: 0.798 acc_test: 0.812\n",
      "epoch:  1736 loss:  0.413 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1737 loss:  0.708 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1738 loss:  0.796 acc_train: 0.796 acc_test: 0.812\n",
      "epoch:  1739 loss:  0.825 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1740 loss:  0.343 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1741 loss:  0.714 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  1742 loss:  0.427 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1743 loss:  0.54 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1744 loss:  0.354 acc_train: 0.777 acc_test: 0.776\n",
      "epoch:  1745 loss:  0.189 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1746 loss:  0.593 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1747 loss:  0.458 acc_train: 0.793 acc_test: 0.789\n",
      "epoch:  1748 loss:  0.261 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1749 loss:  0.489 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1750 loss:  0.679 acc_train: 0.781 acc_test: 0.807\n",
      "epoch:  1751 loss:  0.612 acc_train: 0.787 acc_test: 0.812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1752 loss:  0.287 acc_train: 0.766 acc_test: 0.753\n",
      "epoch:  1753 loss:  0.177 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1754 loss:  0.267 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1755 loss:  0.748 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1756 loss:  0.426 acc_train: 0.775 acc_test: 0.767\n",
      "epoch:  1757 loss:  0.25 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1758 loss:  0.376 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1759 loss:  0.505 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1760 loss:  0.517 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1761 loss:  0.244 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1762 loss:  0.588 acc_train: 0.795 acc_test: 0.816\n",
      "epoch:  1763 loss:  0.481 acc_train: 0.783 acc_test: 0.785\n",
      "epoch:  1764 loss:  0.462 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  1765 loss:  0.304 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1766 loss:  0.604 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1767 loss:  0.492 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1768 loss:  0.385 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1769 loss:  0.697 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1770 loss:  0.362 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1771 loss:  0.694 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1772 loss:  0.352 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1773 loss:  0.292 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1774 loss:  0.482 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1775 loss:  0.294 acc_train: 0.781 acc_test: 0.776\n",
      "epoch:  1776 loss:  0.353 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1777 loss:  0.349 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1778 loss:  0.224 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1779 loss:  0.48 acc_train: 0.772 acc_test: 0.753\n",
      "epoch:  1780 loss:  0.288 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1781 loss:  0.296 acc_train: 0.78 acc_test: 0.803\n",
      "epoch:  1782 loss:  0.502 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1783 loss:  0.466 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  1784 loss:  0.876 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1785 loss:  1.384 acc_train: 0.78 acc_test: 0.785\n",
      "epoch:  1786 loss:  0.507 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1787 loss:  0.246 acc_train: 0.778 acc_test: 0.785\n",
      "epoch:  1788 loss:  0.454 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  1789 loss:  0.508 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1790 loss:  0.54 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1791 loss:  0.647 acc_train: 0.768 acc_test: 0.753\n",
      "epoch:  1792 loss:  0.284 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1793 loss:  0.566 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1794 loss:  0.554 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  1795 loss:  0.733 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1796 loss:  0.552 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1797 loss:  0.507 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  1798 loss:  0.469 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1799 loss:  0.508 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1800 loss:  0.51 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  1801 loss:  0.488 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1802 loss:  0.274 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  1803 loss:  0.783 acc_train: 0.777 acc_test: 0.776\n",
      "epoch:  1804 loss:  0.855 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1805 loss:  0.326 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  1806 loss:  0.433 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1807 loss:  0.352 acc_train: 0.78 acc_test: 0.803\n",
      "epoch:  1808 loss:  0.575 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1809 loss:  0.657 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  1810 loss:  0.379 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  1811 loss:  0.499 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1812 loss:  0.67 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1813 loss:  0.717 acc_train: 0.774 acc_test: 0.753\n",
      "epoch:  1814 loss:  0.357 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1815 loss:  0.602 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1816 loss:  0.828 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1817 loss:  0.406 acc_train: 0.798 acc_test: 0.812\n",
      "epoch:  1818 loss:  0.394 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1819 loss:  0.544 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1820 loss:  0.506 acc_train: 0.799 acc_test: 0.807\n",
      "epoch:  1821 loss:  0.248 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1822 loss:  0.299 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1823 loss:  0.555 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  1824 loss:  0.413 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1825 loss:  0.389 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1826 loss:  0.57 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1827 loss:  0.489 acc_train: 0.798 acc_test: 0.807\n",
      "epoch:  1828 loss:  0.299 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1829 loss:  0.266 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1830 loss:  0.534 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1831 loss:  0.465 acc_train: 0.784 acc_test: 0.794\n",
      "epoch:  1832 loss:  0.514 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1833 loss:  0.465 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  1834 loss:  0.257 acc_train: 0.774 acc_test: 0.767\n",
      "epoch:  1835 loss:  0.53 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1836 loss:  0.774 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  1837 loss:  0.453 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1838 loss:  0.729 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  1839 loss:  0.524 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1840 loss:  0.562 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1841 loss:  0.335 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1842 loss:  0.331 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1843 loss:  0.481 acc_train: 0.768 acc_test: 0.749\n",
      "epoch:  1844 loss:  0.556 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1845 loss:  0.376 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1846 loss:  0.393 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1847 loss:  0.567 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  1848 loss:  0.566 acc_train: 0.778 acc_test: 0.753\n",
      "epoch:  1849 loss:  0.3 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1850 loss:  0.508 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1851 loss:  0.584 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1852 loss:  0.426 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1853 loss:  0.823 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  1854 loss:  0.55 acc_train: 0.78 acc_test: 0.785\n",
      "epoch:  1855 loss:  0.736 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  1856 loss:  0.524 acc_train: 0.777 acc_test: 0.776\n",
      "epoch:  1857 loss:  0.616 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1858 loss:  0.557 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1859 loss:  0.358 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1860 loss:  0.213 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  1861 loss:  0.549 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1862 loss:  0.462 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1863 loss:  0.612 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1864 loss:  0.781 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  1865 loss:  0.668 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1866 loss:  0.466 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1867 loss:  0.386 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1868 loss:  0.358 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1869 loss:  0.69 acc_train: 0.796 acc_test: 0.807\n",
      "epoch:  1870 loss:  0.47 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1871 loss:  0.407 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1872 loss:  0.547 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1873 loss:  0.807 acc_train: 0.784 acc_test: 0.78\n",
      "epoch:  1874 loss:  0.534 acc_train: 0.772 acc_test: 0.767\n",
      "epoch:  1875 loss:  0.266 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1876 loss:  0.707 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1877 loss:  0.544 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1878 loss:  0.366 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1879 loss:  0.66 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1880 loss:  0.374 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  1881 loss:  0.641 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1882 loss:  0.456 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  1883 loss:  0.259 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1884 loss:  0.65 acc_train: 0.777 acc_test: 0.758\n",
      "epoch:  1885 loss:  0.56 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1886 loss:  0.739 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1887 loss:  0.436 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  1888 loss:  0.849 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  1889 loss:  0.352 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  1890 loss:  0.2 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1891 loss:  0.318 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1892 loss:  0.368 acc_train: 0.772 acc_test: 0.767\n",
      "epoch:  1893 loss:  0.871 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1894 loss:  0.504 acc_train: 0.796 acc_test: 0.812\n",
      "epoch:  1895 loss:  0.438 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1896 loss:  0.501 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1897 loss:  0.552 acc_train: 0.771 acc_test: 0.753\n",
      "epoch:  1898 loss:  0.479 acc_train: 0.787 acc_test: 0.803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1899 loss:  0.661 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1900 loss:  0.723 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1901 loss:  0.462 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  1902 loss:  0.238 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1903 loss:  0.758 acc_train: 0.778 acc_test: 0.789\n",
      "epoch:  1904 loss:  0.336 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1905 loss:  0.531 acc_train: 0.772 acc_test: 0.767\n",
      "epoch:  1906 loss:  0.294 acc_train: 0.796 acc_test: 0.807\n",
      "epoch:  1907 loss:  0.524 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1908 loss:  0.401 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1909 loss:  0.337 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1910 loss:  0.45 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  1911 loss:  0.397 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1912 loss:  0.592 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1913 loss:  0.36 acc_train: 0.777 acc_test: 0.753\n",
      "epoch:  1914 loss:  0.248 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1915 loss:  0.641 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1916 loss:  0.517 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  1917 loss:  0.307 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1918 loss:  0.283 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1919 loss:  0.358 acc_train: 0.778 acc_test: 0.758\n",
      "epoch:  1920 loss:  0.192 acc_train: 0.793 acc_test: 0.816\n",
      "epoch:  1921 loss:  0.529 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1922 loss:  0.323 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1923 loss:  0.433 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1924 loss:  0.502 acc_train: 0.765 acc_test: 0.744\n",
      "epoch:  1925 loss:  0.265 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1926 loss:  0.323 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1927 loss:  0.545 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1928 loss:  0.287 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  1929 loss:  0.509 acc_train: 0.771 acc_test: 0.762\n",
      "epoch:  1930 loss:  0.516 acc_train: 0.78 acc_test: 0.803\n",
      "epoch:  1931 loss:  0.419 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1932 loss:  0.327 acc_train: 0.778 acc_test: 0.78\n",
      "epoch:  1933 loss:  0.345 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1934 loss:  0.296 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1935 loss:  0.384 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  1936 loss:  0.455 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1937 loss:  0.351 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  1938 loss:  0.359 acc_train: 0.801 acc_test: 0.812\n",
      "epoch:  1939 loss:  0.436 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1940 loss:  0.536 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1941 loss:  0.196 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  1942 loss:  0.553 acc_train: 0.783 acc_test: 0.776\n",
      "epoch:  1943 loss:  0.558 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1944 loss:  0.411 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1945 loss:  0.309 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1946 loss:  0.602 acc_train: 0.774 acc_test: 0.767\n",
      "epoch:  1947 loss:  0.668 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1948 loss:  0.531 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1949 loss:  0.501 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1950 loss:  0.404 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1951 loss:  0.47 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  1952 loss:  0.463 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  1953 loss:  0.326 acc_train: 0.799 acc_test: 0.821\n",
      "epoch:  1954 loss:  0.693 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  1955 loss:  0.314 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  1956 loss:  0.375 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1957 loss:  0.533 acc_train: 0.78 acc_test: 0.785\n",
      "epoch:  1958 loss:  0.514 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1959 loss:  0.688 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1960 loss:  0.454 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1961 loss:  0.403 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1962 loss:  0.448 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1963 loss:  0.495 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1964 loss:  0.494 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  1965 loss:  0.614 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  1966 loss:  0.35 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1967 loss:  0.407 acc_train: 0.772 acc_test: 0.762\n",
      "epoch:  1968 loss:  0.483 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1969 loss:  0.595 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1970 loss:  0.393 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  1971 loss:  0.383 acc_train: 0.771 acc_test: 0.753\n",
      "epoch:  1972 loss:  0.619 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1973 loss:  0.309 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1974 loss:  0.469 acc_train: 0.777 acc_test: 0.771\n",
      "epoch:  1975 loss:  0.444 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  1976 loss:  0.167 acc_train: 0.778 acc_test: 0.771\n",
      "epoch:  1977 loss:  0.395 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1978 loss:  0.64 acc_train: 0.778 acc_test: 0.758\n",
      "epoch:  1979 loss:  0.448 acc_train: 0.793 acc_test: 0.812\n",
      "epoch:  1980 loss:  0.27 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  1981 loss:  0.751 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  1982 loss:  0.795 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  1983 loss:  0.254 acc_train: 0.772 acc_test: 0.753\n",
      "epoch:  1984 loss:  0.403 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  1985 loss:  0.545 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  1986 loss:  0.601 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  1987 loss:  0.62 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  1988 loss:  0.349 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  1989 loss:  0.521 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  1990 loss:  0.552 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  1991 loss:  0.146 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  1992 loss:  0.26 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1993 loss:  0.443 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  1994 loss:  0.983 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  1995 loss:  0.369 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1996 loss:  0.549 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  1997 loss:  0.521 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  1998 loss:  0.182 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  1999 loss:  0.328 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2000 loss:  0.44 acc_train: 0.786 acc_test: 0.785\n",
      "epoch:  2001 loss:  0.612 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2002 loss:  0.193 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2003 loss:  0.62 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  2004 loss:  0.551 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  2005 loss:  0.497 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2006 loss:  0.572 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  2007 loss:  0.542 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  2008 loss:  0.691 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2009 loss:  0.4 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  2010 loss:  0.382 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2011 loss:  0.21 acc_train: 0.774 acc_test: 0.767\n",
      "epoch:  2012 loss:  0.527 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2013 loss:  0.751 acc_train: 0.763 acc_test: 0.749\n",
      "epoch:  2014 loss:  0.555 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2015 loss:  0.63 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2016 loss:  0.681 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  2017 loss:  0.595 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2018 loss:  0.295 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2019 loss:  0.536 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2020 loss:  0.322 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2021 loss:  0.199 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2022 loss:  0.691 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2023 loss:  0.555 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2024 loss:  0.399 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2025 loss:  0.368 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2026 loss:  0.428 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2027 loss:  0.43 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2028 loss:  0.43 acc_train: 0.78 acc_test: 0.785\n",
      "epoch:  2029 loss:  0.326 acc_train: 0.778 acc_test: 0.771\n",
      "epoch:  2030 loss:  0.326 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2031 loss:  0.342 acc_train: 0.775 acc_test: 0.762\n",
      "epoch:  2032 loss:  0.537 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2033 loss:  0.449 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2034 loss:  0.348 acc_train: 0.775 acc_test: 0.749\n",
      "epoch:  2035 loss:  0.22 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2036 loss:  0.29 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2037 loss:  0.323 acc_train: 0.775 acc_test: 0.771\n",
      "epoch:  2038 loss:  0.309 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2039 loss:  0.816 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2040 loss:  0.754 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2041 loss:  0.386 acc_train: 0.78 acc_test: 0.789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2042 loss:  0.32 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2043 loss:  0.342 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2044 loss:  0.64 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2045 loss:  0.803 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2046 loss:  0.449 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2047 loss:  0.536 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  2048 loss:  0.402 acc_train: 0.798 acc_test: 0.812\n",
      "epoch:  2049 loss:  0.616 acc_train: 0.796 acc_test: 0.803\n",
      "epoch:  2050 loss:  0.365 acc_train: 0.771 acc_test: 0.749\n",
      "epoch:  2051 loss:  0.819 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2052 loss:  0.194 acc_train: 0.777 acc_test: 0.758\n",
      "epoch:  2053 loss:  0.604 acc_train: 0.795 acc_test: 0.816\n",
      "epoch:  2054 loss:  0.418 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2055 loss:  0.589 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2056 loss:  0.59 acc_train: 0.778 acc_test: 0.785\n",
      "epoch:  2057 loss:  0.433 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2058 loss:  0.338 acc_train: 0.777 acc_test: 0.753\n",
      "epoch:  2059 loss:  0.45 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  2060 loss:  0.298 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2061 loss:  1.162 acc_train: 0.778 acc_test: 0.785\n",
      "epoch:  2062 loss:  0.835 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2063 loss:  0.635 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2064 loss:  0.598 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2065 loss:  0.849 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2066 loss:  0.529 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2067 loss:  0.463 acc_train: 0.778 acc_test: 0.771\n",
      "epoch:  2068 loss:  0.392 acc_train: 0.778 acc_test: 0.785\n",
      "epoch:  2069 loss:  0.314 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2070 loss:  0.599 acc_train: 0.801 acc_test: 0.812\n",
      "epoch:  2071 loss:  0.328 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2072 loss:  0.802 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2073 loss:  0.472 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2074 loss:  0.376 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2075 loss:  0.697 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2076 loss:  0.348 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  2077 loss:  0.454 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2078 loss:  0.489 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2079 loss:  0.22 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2080 loss:  0.275 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2081 loss:  0.518 acc_train: 0.786 acc_test: 0.789\n",
      "epoch:  2082 loss:  0.488 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  2083 loss:  0.638 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2084 loss:  0.401 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2085 loss:  0.495 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2086 loss:  0.525 acc_train: 0.774 acc_test: 0.758\n",
      "epoch:  2087 loss:  0.573 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  2088 loss:  0.316 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2089 loss:  0.41 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2090 loss:  0.315 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2091 loss:  0.47 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2092 loss:  0.286 acc_train: 0.78 acc_test: 0.753\n",
      "epoch:  2093 loss:  0.535 acc_train: 0.799 acc_test: 0.807\n",
      "epoch:  2094 loss:  0.614 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2095 loss:  0.297 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2096 loss:  0.351 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2097 loss:  0.428 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2098 loss:  0.442 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2099 loss:  0.288 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2100 loss:  0.366 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2101 loss:  0.283 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2102 loss:  0.552 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  2103 loss:  0.396 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2104 loss:  0.426 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2105 loss:  0.548 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  2106 loss:  0.208 acc_train: 0.795 acc_test: 0.803\n",
      "epoch:  2107 loss:  0.472 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2108 loss:  0.575 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2109 loss:  0.305 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2110 loss:  0.674 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2111 loss:  0.213 acc_train: 0.778 acc_test: 0.785\n",
      "epoch:  2112 loss:  0.686 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2113 loss:  0.308 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2114 loss:  0.448 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2115 loss:  0.44 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2116 loss:  0.442 acc_train: 0.769 acc_test: 0.753\n",
      "epoch:  2117 loss:  0.54 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2118 loss:  0.526 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  2119 loss:  0.561 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2120 loss:  0.236 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2121 loss:  0.57 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  2122 loss:  0.414 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  2123 loss:  0.32 acc_train: 0.778 acc_test: 0.78\n",
      "epoch:  2124 loss:  0.341 acc_train: 0.792 acc_test: 0.803\n",
      "epoch:  2125 loss:  0.608 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2126 loss:  0.435 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2127 loss:  0.424 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  2128 loss:  0.716 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2129 loss:  0.348 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2130 loss:  0.346 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2131 loss:  0.322 acc_train: 0.772 acc_test: 0.767\n",
      "epoch:  2132 loss:  0.628 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2133 loss:  0.869 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2134 loss:  0.465 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2135 loss:  0.502 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2136 loss:  0.953 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  2137 loss:  0.795 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2138 loss:  0.498 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  2139 loss:  0.41 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2140 loss:  0.598 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2141 loss:  0.21 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2142 loss:  0.247 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2143 loss:  0.706 acc_train: 0.777 acc_test: 0.776\n",
      "epoch:  2144 loss:  0.391 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2145 loss:  0.408 acc_train: 0.789 acc_test: 0.816\n",
      "epoch:  2146 loss:  0.224 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2147 loss:  0.469 acc_train: 0.799 acc_test: 0.807\n",
      "epoch:  2148 loss:  0.997 acc_train: 0.766 acc_test: 0.749\n",
      "epoch:  2149 loss:  0.271 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2150 loss:  0.584 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2151 loss:  0.28 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2152 loss:  0.541 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2153 loss:  0.222 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2154 loss:  0.223 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  2155 loss:  0.24 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2156 loss:  0.445 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2157 loss:  0.308 acc_train: 0.778 acc_test: 0.753\n",
      "epoch:  2158 loss:  0.252 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2159 loss:  0.384 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2160 loss:  0.392 acc_train: 0.769 acc_test: 0.744\n",
      "epoch:  2161 loss:  0.439 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  2162 loss:  0.289 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2163 loss:  0.609 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2164 loss:  0.355 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2165 loss:  0.297 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2166 loss:  0.403 acc_train: 0.792 acc_test: 0.803\n",
      "epoch:  2167 loss:  0.374 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2168 loss:  0.315 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2169 loss:  0.478 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2170 loss:  0.393 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2171 loss:  0.449 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2172 loss:  0.694 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2173 loss:  0.439 acc_train: 0.787 acc_test: 0.798\n",
      "epoch:  2174 loss:  0.455 acc_train: 0.774 acc_test: 0.767\n",
      "epoch:  2175 loss:  0.524 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  2176 loss:  0.538 acc_train: 0.778 acc_test: 0.767\n",
      "epoch:  2177 loss:  0.238 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2178 loss:  0.409 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  2179 loss:  0.342 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2180 loss:  0.935 acc_train: 0.792 acc_test: 0.803\n",
      "epoch:  2181 loss:  0.268 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2182 loss:  0.512 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2183 loss:  0.331 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2184 loss:  0.347 acc_train: 0.769 acc_test: 0.749\n",
      "epoch:  2185 loss:  0.509 acc_train: 0.781 acc_test: 0.794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2186 loss:  0.335 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2187 loss:  0.406 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2188 loss:  0.586 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2189 loss:  0.316 acc_train: 0.774 acc_test: 0.758\n",
      "epoch:  2190 loss:  0.608 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2191 loss:  0.333 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2192 loss:  0.308 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2193 loss:  0.226 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2194 loss:  0.566 acc_train: 0.769 acc_test: 0.758\n",
      "epoch:  2195 loss:  0.692 acc_train: 0.784 acc_test: 0.812\n",
      "epoch:  2196 loss:  0.509 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  2197 loss:  0.541 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2198 loss:  0.3 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  2199 loss:  0.383 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2200 loss:  0.281 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2201 loss:  0.415 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2202 loss:  0.612 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2203 loss:  0.383 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2204 loss:  0.382 acc_train: 0.781 acc_test: 0.749\n",
      "epoch:  2205 loss:  0.525 acc_train: 0.78 acc_test: 0.762\n",
      "epoch:  2206 loss:  0.776 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2207 loss:  0.386 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2208 loss:  0.553 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2209 loss:  0.355 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2210 loss:  0.499 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2211 loss:  0.671 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2212 loss:  0.274 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  2213 loss:  0.306 acc_train: 0.787 acc_test: 0.798\n",
      "epoch:  2214 loss:  0.262 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2215 loss:  0.405 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2216 loss:  0.242 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2217 loss:  0.703 acc_train: 0.78 acc_test: 0.803\n",
      "epoch:  2218 loss:  0.182 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  2219 loss:  0.19 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  2220 loss:  0.346 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2221 loss:  0.57 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2222 loss:  0.655 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2223 loss:  0.57 acc_train: 0.792 acc_test: 0.821\n",
      "epoch:  2224 loss:  0.566 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  2225 loss:  0.342 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2226 loss:  0.487 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2227 loss:  0.522 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  2228 loss:  0.518 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2229 loss:  0.364 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2230 loss:  0.517 acc_train: 0.775 acc_test: 0.767\n",
      "epoch:  2231 loss:  0.98 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2232 loss:  0.305 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  2233 loss:  0.396 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2234 loss:  0.328 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  2235 loss:  0.633 acc_train: 0.772 acc_test: 0.762\n",
      "epoch:  2236 loss:  0.276 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2237 loss:  0.667 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2238 loss:  0.424 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2239 loss:  0.334 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2240 loss:  0.684 acc_train: 0.765 acc_test: 0.744\n",
      "epoch:  2241 loss:  0.429 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2242 loss:  0.87 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2243 loss:  0.723 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2244 loss:  0.518 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2245 loss:  0.565 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2246 loss:  0.47 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2247 loss:  0.271 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2248 loss:  0.458 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2249 loss:  0.49 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  2250 loss:  0.49 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2251 loss:  0.477 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2252 loss:  0.456 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  2253 loss:  0.382 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  2254 loss:  0.435 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2255 loss:  0.697 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2256 loss:  0.525 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2257 loss:  0.508 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2258 loss:  0.316 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2259 loss:  0.526 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2260 loss:  0.524 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  2261 loss:  0.555 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2262 loss:  0.207 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2263 loss:  0.234 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2264 loss:  0.45 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2265 loss:  0.589 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  2266 loss:  0.437 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2267 loss:  0.677 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  2268 loss:  0.663 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  2269 loss:  0.494 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2270 loss:  0.274 acc_train: 0.796 acc_test: 0.807\n",
      "epoch:  2271 loss:  0.491 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2272 loss:  0.525 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2273 loss:  0.206 acc_train: 0.786 acc_test: 0.785\n",
      "epoch:  2274 loss:  0.75 acc_train: 0.783 acc_test: 0.794\n",
      "epoch:  2275 loss:  0.583 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2276 loss:  0.589 acc_train: 0.78 acc_test: 0.803\n",
      "epoch:  2277 loss:  0.6 acc_train: 0.772 acc_test: 0.762\n",
      "epoch:  2278 loss:  0.511 acc_train: 0.78 acc_test: 0.785\n",
      "epoch:  2279 loss:  0.564 acc_train: 0.777 acc_test: 0.762\n",
      "epoch:  2280 loss:  0.258 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2281 loss:  0.219 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2282 loss:  0.865 acc_train: 0.792 acc_test: 0.803\n",
      "epoch:  2283 loss:  0.266 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2284 loss:  0.618 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2285 loss:  0.32 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  2286 loss:  0.701 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2287 loss:  0.382 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2288 loss:  0.587 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2289 loss:  0.538 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2290 loss:  0.462 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  2291 loss:  0.448 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2292 loss:  0.428 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  2293 loss:  0.256 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  2294 loss:  0.662 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2295 loss:  0.327 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2296 loss:  0.479 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2297 loss:  0.355 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2298 loss:  0.553 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2299 loss:  0.361 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2300 loss:  0.324 acc_train: 0.774 acc_test: 0.767\n",
      "epoch:  2301 loss:  0.51 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2302 loss:  0.383 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2303 loss:  0.628 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2304 loss:  0.248 acc_train: 0.798 acc_test: 0.812\n",
      "epoch:  2305 loss:  0.65 acc_train: 0.774 acc_test: 0.758\n",
      "epoch:  2306 loss:  0.327 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2307 loss:  0.385 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2308 loss:  0.244 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2309 loss:  0.288 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  2310 loss:  0.705 acc_train: 0.777 acc_test: 0.758\n",
      "epoch:  2311 loss:  0.34 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2312 loss:  0.668 acc_train: 0.792 acc_test: 0.821\n",
      "epoch:  2313 loss:  0.505 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2314 loss:  0.426 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  2315 loss:  0.516 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2316 loss:  0.344 acc_train: 0.774 acc_test: 0.762\n",
      "epoch:  2317 loss:  0.349 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2318 loss:  0.498 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2319 loss:  0.616 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2320 loss:  0.291 acc_train: 0.783 acc_test: 0.785\n",
      "epoch:  2321 loss:  0.301 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2322 loss:  0.293 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2323 loss:  0.478 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2324 loss:  0.295 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  2325 loss:  0.729 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2326 loss:  0.504 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2327 loss:  0.424 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  2328 loss:  0.442 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  2329 loss:  0.42 acc_train: 0.787 acc_test: 0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2330 loss:  0.301 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2331 loss:  0.501 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  2332 loss:  0.189 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2333 loss:  0.496 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2334 loss:  0.478 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2335 loss:  0.392 acc_train: 0.772 acc_test: 0.762\n",
      "epoch:  2336 loss:  0.661 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2337 loss:  0.504 acc_train: 0.777 acc_test: 0.776\n",
      "epoch:  2338 loss:  0.738 acc_train: 0.793 acc_test: 0.816\n",
      "epoch:  2339 loss:  0.435 acc_train: 0.778 acc_test: 0.771\n",
      "epoch:  2340 loss:  0.481 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2341 loss:  0.459 acc_train: 0.796 acc_test: 0.807\n",
      "epoch:  2342 loss:  0.579 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2343 loss:  0.433 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2344 loss:  0.569 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2345 loss:  0.916 acc_train: 0.781 acc_test: 0.794\n",
      "epoch:  2346 loss:  0.217 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2347 loss:  0.708 acc_train: 0.781 acc_test: 0.771\n",
      "epoch:  2348 loss:  0.596 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2349 loss:  0.457 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2350 loss:  0.619 acc_train: 0.781 acc_test: 0.785\n",
      "epoch:  2351 loss:  0.448 acc_train: 0.78 acc_test: 0.758\n",
      "epoch:  2352 loss:  0.541 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2353 loss:  0.431 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2354 loss:  0.456 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2355 loss:  0.482 acc_train: 0.777 acc_test: 0.762\n",
      "epoch:  2356 loss:  0.403 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  2357 loss:  0.64 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2358 loss:  0.309 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2359 loss:  0.403 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  2360 loss:  0.631 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2361 loss:  0.46 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  2362 loss:  0.535 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2363 loss:  0.567 acc_train: 0.793 acc_test: 0.812\n",
      "epoch:  2364 loss:  0.312 acc_train: 0.787 acc_test: 0.798\n",
      "epoch:  2365 loss:  0.296 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2366 loss:  0.221 acc_train: 0.772 acc_test: 0.758\n",
      "epoch:  2367 loss:  0.308 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2368 loss:  0.457 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2369 loss:  0.867 acc_train: 0.777 acc_test: 0.758\n",
      "epoch:  2370 loss:  0.238 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  2371 loss:  0.397 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2372 loss:  0.572 acc_train: 0.78 acc_test: 0.785\n",
      "epoch:  2373 loss:  0.285 acc_train: 0.781 acc_test: 0.794\n",
      "epoch:  2374 loss:  0.36 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2375 loss:  0.302 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2376 loss:  0.194 acc_train: 0.781 acc_test: 0.794\n",
      "epoch:  2377 loss:  0.551 acc_train: 0.799 acc_test: 0.807\n",
      "epoch:  2378 loss:  0.561 acc_train: 0.784 acc_test: 0.812\n",
      "epoch:  2379 loss:  0.526 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  2380 loss:  0.376 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2381 loss:  0.586 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2382 loss:  0.613 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2383 loss:  0.41 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2384 loss:  0.468 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2385 loss:  0.332 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2386 loss:  0.746 acc_train: 0.787 acc_test: 0.789\n",
      "epoch:  2387 loss:  0.789 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  2388 loss:  0.764 acc_train: 0.777 acc_test: 0.758\n",
      "epoch:  2389 loss:  0.276 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2390 loss:  0.609 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2391 loss:  0.282 acc_train: 0.781 acc_test: 0.771\n",
      "epoch:  2392 loss:  0.297 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2393 loss:  0.281 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2394 loss:  0.343 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2395 loss:  0.343 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2396 loss:  0.853 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2397 loss:  0.572 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2398 loss:  0.719 acc_train: 0.798 acc_test: 0.807\n",
      "epoch:  2399 loss:  0.222 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2400 loss:  0.422 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  2401 loss:  0.459 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2402 loss:  0.454 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2403 loss:  0.764 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2404 loss:  0.714 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2405 loss:  0.345 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  2406 loss:  0.646 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2407 loss:  0.901 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2408 loss:  0.696 acc_train: 0.777 acc_test: 0.758\n",
      "epoch:  2409 loss:  0.371 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  2410 loss:  0.328 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2411 loss:  0.286 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2412 loss:  0.666 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2413 loss:  0.428 acc_train: 0.766 acc_test: 0.744\n",
      "epoch:  2414 loss:  0.336 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2415 loss:  0.325 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2416 loss:  0.395 acc_train: 0.781 acc_test: 0.807\n",
      "epoch:  2417 loss:  0.674 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2418 loss:  0.89 acc_train: 0.781 acc_test: 0.785\n",
      "epoch:  2419 loss:  0.381 acc_train: 0.781 acc_test: 0.807\n",
      "epoch:  2420 loss:  0.563 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2421 loss:  0.34 acc_train: 0.78 acc_test: 0.785\n",
      "epoch:  2422 loss:  0.413 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2423 loss:  0.559 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2424 loss:  0.45 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2425 loss:  0.607 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2426 loss:  0.64 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  2427 loss:  0.472 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2428 loss:  0.292 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2429 loss:  0.41 acc_train: 0.796 acc_test: 0.807\n",
      "epoch:  2430 loss:  0.528 acc_train: 0.798 acc_test: 0.803\n",
      "epoch:  2431 loss:  0.406 acc_train: 0.774 acc_test: 0.762\n",
      "epoch:  2432 loss:  0.197 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2433 loss:  0.203 acc_train: 0.787 acc_test: 0.798\n",
      "epoch:  2434 loss:  0.466 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2435 loss:  0.299 acc_train: 0.771 acc_test: 0.758\n",
      "epoch:  2436 loss:  0.428 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  2437 loss:  0.333 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2438 loss:  0.199 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2439 loss:  0.522 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2440 loss:  0.477 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2441 loss:  0.56 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2442 loss:  0.64 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2443 loss:  0.306 acc_train: 0.796 acc_test: 0.807\n",
      "epoch:  2444 loss:  0.37 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2445 loss:  0.375 acc_train: 0.769 acc_test: 0.749\n",
      "epoch:  2446 loss:  0.247 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2447 loss:  0.385 acc_train: 0.79 acc_test: 0.821\n",
      "epoch:  2448 loss:  0.645 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2449 loss:  0.485 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2450 loss:  0.488 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  2451 loss:  0.395 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2452 loss:  0.375 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2453 loss:  0.377 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2454 loss:  0.462 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2455 loss:  0.585 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2456 loss:  0.331 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2457 loss:  0.355 acc_train: 0.793 acc_test: 0.812\n",
      "epoch:  2458 loss:  0.834 acc_train: 0.769 acc_test: 0.749\n",
      "epoch:  2459 loss:  0.518 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2460 loss:  0.721 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  2461 loss:  0.459 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2462 loss:  1.334 acc_train: 0.777 acc_test: 0.762\n",
      "epoch:  2463 loss:  0.252 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2464 loss:  0.492 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2465 loss:  0.836 acc_train: 0.777 acc_test: 0.767\n",
      "epoch:  2466 loss:  0.413 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2467 loss:  0.591 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  2468 loss:  0.559 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2469 loss:  0.55 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2470 loss:  0.326 acc_train: 0.777 acc_test: 0.762\n",
      "epoch:  2471 loss:  0.358 acc_train: 0.796 acc_test: 0.812\n",
      "epoch:  2472 loss:  0.296 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  2473 loss:  0.478 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2474 loss:  0.289 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2475 loss:  0.85 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2476 loss:  0.401 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2477 loss:  0.3 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2478 loss:  0.386 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2479 loss:  0.564 acc_train: 0.787 acc_test: 0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2480 loss:  0.36 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2481 loss:  0.42 acc_train: 0.775 acc_test: 0.767\n",
      "epoch:  2482 loss:  0.487 acc_train: 0.798 acc_test: 0.816\n",
      "epoch:  2483 loss:  0.555 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2484 loss:  0.305 acc_train: 0.777 acc_test: 0.78\n",
      "epoch:  2485 loss:  0.363 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2486 loss:  0.556 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2487 loss:  0.481 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  2488 loss:  0.453 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2489 loss:  0.313 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2490 loss:  0.31 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2491 loss:  0.633 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2492 loss:  0.414 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2493 loss:  0.689 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2494 loss:  0.354 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2495 loss:  0.351 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  2496 loss:  0.499 acc_train: 0.78 acc_test: 0.785\n",
      "epoch:  2497 loss:  0.198 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2498 loss:  0.33 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2499 loss:  0.48 acc_train: 0.789 acc_test: 0.798\n",
      "epoch:  2500 loss:  0.384 acc_train: 0.778 acc_test: 0.803\n",
      "epoch:  2501 loss:  0.675 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  2502 loss:  0.368 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2503 loss:  0.347 acc_train: 0.777 acc_test: 0.78\n",
      "epoch:  2504 loss:  0.547 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  2505 loss:  0.243 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2506 loss:  0.681 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2507 loss:  0.32 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2508 loss:  0.956 acc_train: 0.746 acc_test: 0.744\n",
      "epoch:  2509 loss:  0.672 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2510 loss:  0.39 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2511 loss:  0.179 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2512 loss:  0.638 acc_train: 0.774 acc_test: 0.767\n",
      "epoch:  2513 loss:  0.408 acc_train: 0.778 acc_test: 0.789\n",
      "epoch:  2514 loss:  0.404 acc_train: 0.774 acc_test: 0.762\n",
      "epoch:  2515 loss:  0.612 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2516 loss:  0.409 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  2517 loss:  0.542 acc_train: 0.796 acc_test: 0.812\n",
      "epoch:  2518 loss:  0.361 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2519 loss:  0.437 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2520 loss:  0.279 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2521 loss:  0.519 acc_train: 0.774 acc_test: 0.767\n",
      "epoch:  2522 loss:  0.287 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2523 loss:  0.454 acc_train: 0.781 acc_test: 0.794\n",
      "epoch:  2524 loss:  0.301 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2525 loss:  0.244 acc_train: 0.775 acc_test: 0.767\n",
      "epoch:  2526 loss:  0.915 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2527 loss:  0.259 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2528 loss:  0.472 acc_train: 0.775 acc_test: 0.762\n",
      "epoch:  2529 loss:  0.67 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2530 loss:  0.465 acc_train: 0.789 acc_test: 0.816\n",
      "epoch:  2531 loss:  0.43 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2532 loss:  0.575 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2533 loss:  0.399 acc_train: 0.778 acc_test: 0.767\n",
      "epoch:  2534 loss:  0.611 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2535 loss:  0.521 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2536 loss:  0.593 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2537 loss:  0.39 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2538 loss:  0.562 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  2539 loss:  0.205 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2540 loss:  0.738 acc_train: 0.793 acc_test: 0.816\n",
      "epoch:  2541 loss:  0.282 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2542 loss:  0.371 acc_train: 0.768 acc_test: 0.744\n",
      "epoch:  2543 loss:  0.562 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  2544 loss:  0.844 acc_train: 0.784 acc_test: 0.78\n",
      "epoch:  2545 loss:  0.513 acc_train: 0.777 acc_test: 0.785\n",
      "epoch:  2546 loss:  0.371 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2547 loss:  0.611 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2548 loss:  0.604 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2549 loss:  0.436 acc_train: 0.793 acc_test: 0.812\n",
      "epoch:  2550 loss:  0.467 acc_train: 0.781 acc_test: 0.776\n",
      "epoch:  2551 loss:  0.681 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  2552 loss:  0.715 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  2553 loss:  0.386 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2554 loss:  0.415 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2555 loss:  0.526 acc_train: 0.781 acc_test: 0.767\n",
      "epoch:  2556 loss:  0.486 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2557 loss:  0.451 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2558 loss:  0.141 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2559 loss:  0.6 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2560 loss:  0.741 acc_train: 0.795 acc_test: 0.816\n",
      "epoch:  2561 loss:  0.528 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2562 loss:  0.422 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2563 loss:  0.339 acc_train: 0.766 acc_test: 0.744\n",
      "epoch:  2564 loss:  0.334 acc_train: 0.781 acc_test: 0.794\n",
      "epoch:  2565 loss:  0.7 acc_train: 0.777 acc_test: 0.762\n",
      "epoch:  2566 loss:  0.824 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2567 loss:  0.351 acc_train: 0.78 acc_test: 0.767\n",
      "epoch:  2568 loss:  0.402 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2569 loss:  0.706 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2570 loss:  0.596 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2571 loss:  0.715 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2572 loss:  0.564 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2573 loss:  0.429 acc_train: 0.781 acc_test: 0.794\n",
      "epoch:  2574 loss:  0.325 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2575 loss:  0.254 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2576 loss:  0.494 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2577 loss:  0.289 acc_train: 0.769 acc_test: 0.749\n",
      "epoch:  2578 loss:  0.661 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2579 loss:  0.487 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2580 loss:  0.657 acc_train: 0.783 acc_test: 0.794\n",
      "epoch:  2581 loss:  0.682 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2582 loss:  0.799 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  2583 loss:  0.549 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2584 loss:  0.551 acc_train: 0.799 acc_test: 0.807\n",
      "epoch:  2585 loss:  0.473 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2586 loss:  0.834 acc_train: 0.777 acc_test: 0.758\n",
      "epoch:  2587 loss:  0.396 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2588 loss:  0.388 acc_train: 0.798 acc_test: 0.812\n",
      "epoch:  2589 loss:  0.404 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2590 loss:  0.717 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2591 loss:  0.586 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2592 loss:  0.745 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2593 loss:  0.453 acc_train: 0.777 acc_test: 0.758\n",
      "epoch:  2594 loss:  0.522 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2595 loss:  0.548 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2596 loss:  0.41 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  2597 loss:  0.457 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2598 loss:  0.568 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2599 loss:  0.493 acc_train: 0.78 acc_test: 0.762\n",
      "epoch:  2600 loss:  0.721 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  2601 loss:  0.4 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2602 loss:  0.57 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2603 loss:  0.276 acc_train: 0.777 acc_test: 0.78\n",
      "epoch:  2604 loss:  0.374 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2605 loss:  0.936 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2606 loss:  0.333 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  2607 loss:  0.47 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2608 loss:  0.343 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2609 loss:  0.49 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2610 loss:  0.481 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  2611 loss:  0.369 acc_train: 0.772 acc_test: 0.749\n",
      "epoch:  2612 loss:  0.409 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2613 loss:  0.246 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2614 loss:  0.301 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2615 loss:  0.338 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2616 loss:  0.476 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  2617 loss:  0.292 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  2618 loss:  0.488 acc_train: 0.799 acc_test: 0.812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2619 loss:  0.364 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2620 loss:  0.186 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2621 loss:  0.909 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  2622 loss:  0.49 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2623 loss:  0.191 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2624 loss:  0.454 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2625 loss:  0.801 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2626 loss:  0.388 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2627 loss:  0.413 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2628 loss:  0.766 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2629 loss:  0.354 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2630 loss:  0.4 acc_train: 0.772 acc_test: 0.749\n",
      "epoch:  2631 loss:  0.548 acc_train: 0.781 acc_test: 0.794\n",
      "epoch:  2632 loss:  0.506 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2633 loss:  0.37 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2634 loss:  0.467 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  2635 loss:  0.694 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2636 loss:  0.265 acc_train: 0.777 acc_test: 0.78\n",
      "epoch:  2637 loss:  0.172 acc_train: 0.79 acc_test: 0.821\n",
      "epoch:  2638 loss:  0.421 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2639 loss:  0.392 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2640 loss:  0.515 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2641 loss:  0.362 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2642 loss:  0.256 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2643 loss:  0.414 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2644 loss:  0.447 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  2645 loss:  0.299 acc_train: 0.78 acc_test: 0.762\n",
      "epoch:  2646 loss:  0.262 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2647 loss:  0.627 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  2648 loss:  0.471 acc_train: 0.781 acc_test: 0.794\n",
      "epoch:  2649 loss:  0.48 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  2650 loss:  0.772 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2651 loss:  0.264 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2652 loss:  0.402 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2653 loss:  0.332 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2654 loss:  0.428 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2655 loss:  0.689 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  2656 loss:  0.513 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  2657 loss:  0.474 acc_train: 0.783 acc_test: 0.794\n",
      "epoch:  2658 loss:  0.315 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2659 loss:  0.261 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  2660 loss:  0.472 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2661 loss:  0.709 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  2662 loss:  0.342 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2663 loss:  0.2 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2664 loss:  0.379 acc_train: 0.789 acc_test: 0.821\n",
      "epoch:  2665 loss:  0.206 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2666 loss:  0.396 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2667 loss:  0.409 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2668 loss:  0.337 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2669 loss:  0.544 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2670 loss:  0.431 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2671 loss:  0.576 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2672 loss:  0.733 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2673 loss:  0.517 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2674 loss:  0.431 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2675 loss:  0.767 acc_train: 0.783 acc_test: 0.798\n",
      "epoch:  2676 loss:  0.536 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2677 loss:  0.769 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2678 loss:  0.497 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2679 loss:  0.508 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2680 loss:  0.411 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2681 loss:  0.465 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2682 loss:  0.233 acc_train: 0.778 acc_test: 0.785\n",
      "epoch:  2683 loss:  0.517 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2684 loss:  0.57 acc_train: 0.798 acc_test: 0.807\n",
      "epoch:  2685 loss:  0.5 acc_train: 0.781 acc_test: 0.794\n",
      "epoch:  2686 loss:  0.308 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2687 loss:  0.497 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  2688 loss:  0.436 acc_train: 0.784 acc_test: 0.78\n",
      "epoch:  2689 loss:  0.346 acc_train: 0.777 acc_test: 0.758\n",
      "epoch:  2690 loss:  0.455 acc_train: 0.781 acc_test: 0.807\n",
      "epoch:  2691 loss:  0.402 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2692 loss:  0.572 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2693 loss:  0.383 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2694 loss:  0.428 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2695 loss:  0.447 acc_train: 0.778 acc_test: 0.758\n",
      "epoch:  2696 loss:  0.956 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2697 loss:  0.475 acc_train: 0.777 acc_test: 0.758\n",
      "epoch:  2698 loss:  0.658 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  2699 loss:  0.422 acc_train: 0.772 acc_test: 0.758\n",
      "epoch:  2700 loss:  0.4 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2701 loss:  0.527 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2702 loss:  0.269 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2703 loss:  0.816 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2704 loss:  0.291 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  2705 loss:  0.393 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2706 loss:  0.21 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2707 loss:  0.264 acc_train: 0.793 acc_test: 0.816\n",
      "epoch:  2708 loss:  0.337 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2709 loss:  0.494 acc_train: 0.769 acc_test: 0.744\n",
      "epoch:  2710 loss:  0.314 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2711 loss:  0.352 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  2712 loss:  0.715 acc_train: 0.772 acc_test: 0.749\n",
      "epoch:  2713 loss:  0.209 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2714 loss:  0.368 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  2715 loss:  0.249 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2716 loss:  0.381 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2717 loss:  0.674 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2718 loss:  0.446 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2719 loss:  1.027 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2720 loss:  0.454 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2721 loss:  0.346 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2722 loss:  0.547 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2723 loss:  0.206 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2724 loss:  0.233 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2725 loss:  0.398 acc_train: 0.774 acc_test: 0.767\n",
      "epoch:  2726 loss:  0.192 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2727 loss:  0.573 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2728 loss:  0.864 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2729 loss:  0.769 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2730 loss:  0.291 acc_train: 0.771 acc_test: 0.749\n",
      "epoch:  2731 loss:  0.67 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2732 loss:  0.505 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2733 loss:  0.589 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2734 loss:  0.549 acc_train: 0.78 acc_test: 0.776\n",
      "epoch:  2735 loss:  0.557 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2736 loss:  0.365 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2737 loss:  0.478 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2738 loss:  0.456 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2739 loss:  0.34 acc_train: 0.796 acc_test: 0.807\n",
      "epoch:  2740 loss:  0.476 acc_train: 0.777 acc_test: 0.78\n",
      "epoch:  2741 loss:  0.789 acc_train: 0.798 acc_test: 0.807\n",
      "epoch:  2742 loss:  0.404 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2743 loss:  0.201 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2744 loss:  0.271 acc_train: 0.775 acc_test: 0.767\n",
      "epoch:  2745 loss:  0.5 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2746 loss:  0.503 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2747 loss:  0.315 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2748 loss:  0.343 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  2749 loss:  0.985 acc_train: 0.78 acc_test: 0.803\n",
      "epoch:  2750 loss:  0.358 acc_train: 0.777 acc_test: 0.762\n",
      "epoch:  2751 loss:  0.48 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2752 loss:  0.319 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  2753 loss:  0.597 acc_train: 0.789 acc_test: 0.812\n",
      "epoch:  2754 loss:  0.721 acc_train: 0.793 acc_test: 0.816\n",
      "epoch:  2755 loss:  0.398 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2756 loss:  0.327 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2757 loss:  0.638 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2758 loss:  0.542 acc_train: 0.783 acc_test: 0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2759 loss:  0.737 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2760 loss:  0.671 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2761 loss:  0.477 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2762 loss:  0.254 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2763 loss:  0.254 acc_train: 0.781 acc_test: 0.794\n",
      "epoch:  2764 loss:  0.391 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  2765 loss:  0.486 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2766 loss:  0.489 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2767 loss:  0.686 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2768 loss:  0.616 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2769 loss:  0.365 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2770 loss:  0.524 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2771 loss:  0.219 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2772 loss:  0.455 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2773 loss:  0.385 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  2774 loss:  0.249 acc_train: 0.789 acc_test: 0.816\n",
      "epoch:  2775 loss:  0.544 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2776 loss:  0.962 acc_train: 0.781 acc_test: 0.776\n",
      "epoch:  2777 loss:  0.367 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2778 loss:  0.802 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  2779 loss:  0.459 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2780 loss:  0.659 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2781 loss:  0.405 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2782 loss:  0.77 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2783 loss:  0.864 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2784 loss:  0.652 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2785 loss:  0.698 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2786 loss:  0.587 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2787 loss:  0.237 acc_train: 0.778 acc_test: 0.758\n",
      "epoch:  2788 loss:  0.573 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  2789 loss:  0.666 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2790 loss:  0.804 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2791 loss:  0.43 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2792 loss:  0.173 acc_train: 0.793 acc_test: 0.812\n",
      "epoch:  2793 loss:  0.511 acc_train: 0.793 acc_test: 0.807\n",
      "epoch:  2794 loss:  0.289 acc_train: 0.762 acc_test: 0.749\n",
      "epoch:  2795 loss:  0.22 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2796 loss:  0.486 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2797 loss:  0.572 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2798 loss:  0.403 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2799 loss:  0.419 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2800 loss:  0.289 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2801 loss:  0.768 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2802 loss:  0.607 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2803 loss:  0.504 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2804 loss:  0.544 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2805 loss:  0.375 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2806 loss:  0.437 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2807 loss:  0.487 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2808 loss:  0.447 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2809 loss:  0.523 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2810 loss:  0.442 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2811 loss:  0.326 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  2812 loss:  0.546 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2813 loss:  0.484 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2814 loss:  0.646 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2815 loss:  0.768 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2816 loss:  0.686 acc_train: 0.801 acc_test: 0.803\n",
      "epoch:  2817 loss:  0.597 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2818 loss:  0.434 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  2819 loss:  0.286 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2820 loss:  0.647 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  2821 loss:  0.358 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2822 loss:  0.254 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2823 loss:  0.468 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2824 loss:  0.644 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2825 loss:  0.482 acc_train: 0.775 acc_test: 0.767\n",
      "epoch:  2826 loss:  0.533 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  2827 loss:  0.218 acc_train: 0.796 acc_test: 0.812\n",
      "epoch:  2828 loss:  0.617 acc_train: 0.784 acc_test: 0.812\n",
      "epoch:  2829 loss:  0.334 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2830 loss:  0.505 acc_train: 0.799 acc_test: 0.812\n",
      "epoch:  2831 loss:  0.39 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  2832 loss:  0.476 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2833 loss:  0.358 acc_train: 0.793 acc_test: 0.812\n",
      "epoch:  2834 loss:  0.809 acc_train: 0.781 acc_test: 0.771\n",
      "epoch:  2835 loss:  0.338 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2836 loss:  0.505 acc_train: 0.793 acc_test: 0.816\n",
      "epoch:  2837 loss:  0.411 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2838 loss:  0.383 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2839 loss:  0.361 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2840 loss:  0.301 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2841 loss:  0.693 acc_train: 0.784 acc_test: 0.798\n",
      "epoch:  2842 loss:  0.484 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2843 loss:  0.46 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  2844 loss:  0.421 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  2845 loss:  0.585 acc_train: 0.781 acc_test: 0.794\n",
      "epoch:  2846 loss:  0.61 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2847 loss:  0.719 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  2848 loss:  0.805 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2849 loss:  0.214 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2850 loss:  0.541 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2851 loss:  0.19 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2852 loss:  0.337 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2853 loss:  0.47 acc_train: 0.781 acc_test: 0.807\n",
      "epoch:  2854 loss:  0.417 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2855 loss:  0.541 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2856 loss:  0.565 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2857 loss:  0.399 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2858 loss:  0.204 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2859 loss:  0.713 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  2860 loss:  0.826 acc_train: 0.786 acc_test: 0.794\n",
      "epoch:  2861 loss:  0.394 acc_train: 0.787 acc_test: 0.812\n",
      "epoch:  2862 loss:  0.613 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2863 loss:  0.397 acc_train: 0.79 acc_test: 0.803\n",
      "epoch:  2864 loss:  0.437 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2865 loss:  0.41 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2866 loss:  0.342 acc_train: 0.793 acc_test: 0.812\n",
      "epoch:  2867 loss:  0.492 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2868 loss:  0.618 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2869 loss:  0.241 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2870 loss:  0.592 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  2871 loss:  0.759 acc_train: 0.796 acc_test: 0.807\n",
      "epoch:  2872 loss:  0.227 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2873 loss:  0.695 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  2874 loss:  0.323 acc_train: 0.783 acc_test: 0.807\n",
      "epoch:  2875 loss:  0.305 acc_train: 0.798 acc_test: 0.807\n",
      "epoch:  2876 loss:  0.614 acc_train: 0.79 acc_test: 0.816\n",
      "epoch:  2877 loss:  0.249 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2878 loss:  0.614 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2879 loss:  0.63 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2880 loss:  0.802 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  2881 loss:  0.273 acc_train: 0.786 acc_test: 0.798\n",
      "epoch:  2882 loss:  0.39 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2883 loss:  0.352 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2884 loss:  0.483 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2885 loss:  0.432 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2886 loss:  0.416 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2887 loss:  0.548 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2888 loss:  0.515 acc_train: 0.796 acc_test: 0.807\n",
      "epoch:  2889 loss:  0.515 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2890 loss:  0.329 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2891 loss:  0.275 acc_train: 0.79 acc_test: 0.812\n",
      "epoch:  2892 loss:  0.15 acc_train: 0.781 acc_test: 0.798\n",
      "epoch:  2893 loss:  0.498 acc_train: 0.778 acc_test: 0.789\n",
      "epoch:  2894 loss:  0.528 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  2895 loss:  0.802 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2896 loss:  0.468 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2897 loss:  0.423 acc_train: 0.796 acc_test: 0.812\n",
      "epoch:  2898 loss:  0.385 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2899 loss:  0.358 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2900 loss:  0.331 acc_train: 0.789 acc_test: 0.789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2901 loss:  0.423 acc_train: 0.787 acc_test: 0.798\n",
      "epoch:  2902 loss:  0.391 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2903 loss:  0.263 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2904 loss:  0.382 acc_train: 0.78 acc_test: 0.798\n",
      "epoch:  2905 loss:  0.389 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2906 loss:  0.601 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2907 loss:  0.903 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2908 loss:  0.612 acc_train: 0.781 acc_test: 0.803\n",
      "epoch:  2909 loss:  0.67 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2910 loss:  0.795 acc_train: 0.778 acc_test: 0.794\n",
      "epoch:  2911 loss:  0.367 acc_train: 0.777 acc_test: 0.758\n",
      "epoch:  2912 loss:  0.493 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2913 loss:  0.805 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2914 loss:  0.286 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2915 loss:  0.686 acc_train: 0.78 acc_test: 0.794\n",
      "epoch:  2916 loss:  0.67 acc_train: 0.796 acc_test: 0.816\n",
      "epoch:  2917 loss:  0.408 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2918 loss:  0.264 acc_train: 0.786 acc_test: 0.816\n",
      "epoch:  2919 loss:  0.386 acc_train: 0.777 acc_test: 0.794\n",
      "epoch:  2920 loss:  0.444 acc_train: 0.777 acc_test: 0.78\n",
      "epoch:  2921 loss:  0.278 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2922 loss:  0.265 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2923 loss:  0.664 acc_train: 0.775 acc_test: 0.758\n",
      "epoch:  2924 loss:  0.448 acc_train: 0.787 acc_test: 0.816\n",
      "epoch:  2925 loss:  0.909 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2926 loss:  0.38 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2927 loss:  0.639 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2928 loss:  0.253 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2929 loss:  0.427 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2930 loss:  0.681 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2931 loss:  0.325 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2932 loss:  0.545 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2933 loss:  0.682 acc_train: 0.792 acc_test: 0.816\n",
      "epoch:  2934 loss:  0.628 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2935 loss:  0.26 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2936 loss:  0.596 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2937 loss:  0.517 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2938 loss:  0.507 acc_train: 0.796 acc_test: 0.803\n",
      "epoch:  2939 loss:  0.583 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2940 loss:  0.37 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2941 loss:  0.243 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2942 loss:  0.494 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2943 loss:  0.735 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2944 loss:  0.551 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2945 loss:  0.181 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2946 loss:  0.262 acc_train: 0.789 acc_test: 0.803\n",
      "epoch:  2947 loss:  0.292 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2948 loss:  0.595 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2949 loss:  0.757 acc_train: 0.78 acc_test: 0.789\n",
      "epoch:  2950 loss:  0.284 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2951 loss:  0.714 acc_train: 0.792 acc_test: 0.807\n",
      "epoch:  2952 loss:  0.242 acc_train: 0.778 acc_test: 0.794\n",
      "epoch:  2953 loss:  0.273 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2954 loss:  0.923 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2955 loss:  0.164 acc_train: 0.799 acc_test: 0.812\n",
      "epoch:  2956 loss:  0.455 acc_train: 0.795 acc_test: 0.812\n",
      "epoch:  2957 loss:  0.448 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2958 loss:  0.405 acc_train: 0.787 acc_test: 0.807\n",
      "epoch:  2959 loss:  0.367 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2960 loss:  0.235 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2961 loss:  0.494 acc_train: 0.795 acc_test: 0.816\n",
      "epoch:  2962 loss:  0.376 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2963 loss:  0.236 acc_train: 0.781 acc_test: 0.789\n",
      "epoch:  2964 loss:  0.427 acc_train: 0.796 acc_test: 0.803\n",
      "epoch:  2965 loss:  0.769 acc_train: 0.774 acc_test: 0.749\n",
      "epoch:  2966 loss:  0.573 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2967 loss:  0.688 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2968 loss:  0.269 acc_train: 0.781 acc_test: 0.771\n",
      "epoch:  2969 loss:  0.473 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2970 loss:  0.669 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2971 loss:  0.281 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2972 loss:  0.733 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2973 loss:  0.473 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2974 loss:  0.49 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2975 loss:  0.969 acc_train: 0.783 acc_test: 0.803\n",
      "epoch:  2976 loss:  0.393 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2977 loss:  0.895 acc_train: 0.787 acc_test: 0.789\n",
      "epoch:  2978 loss:  0.453 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2979 loss:  0.639 acc_train: 0.793 acc_test: 0.816\n",
      "epoch:  2980 loss:  0.251 acc_train: 0.786 acc_test: 0.812\n",
      "epoch:  2981 loss:  0.29 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2982 loss:  0.898 acc_train: 0.772 acc_test: 0.749\n",
      "epoch:  2983 loss:  0.231 acc_train: 0.784 acc_test: 0.807\n",
      "epoch:  2984 loss:  0.333 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2985 loss:  0.647 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2986 loss:  0.326 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2987 loss:  0.57 acc_train: 0.789 acc_test: 0.807\n",
      "epoch:  2988 loss:  0.305 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2989 loss:  0.399 acc_train: 0.783 acc_test: 0.789\n",
      "epoch:  2990 loss:  0.597 acc_train: 0.783 acc_test: 0.794\n",
      "epoch:  2991 loss:  0.384 acc_train: 0.781 acc_test: 0.771\n",
      "epoch:  2992 loss:  0.441 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2993 loss:  0.484 acc_train: 0.792 acc_test: 0.812\n",
      "epoch:  2994 loss:  0.237 acc_train: 0.784 acc_test: 0.803\n",
      "epoch:  2995 loss:  0.321 acc_train: 0.79 acc_test: 0.807\n",
      "epoch:  2996 loss:  0.4 acc_train: 0.786 acc_test: 0.803\n",
      "epoch:  2997 loss:  0.459 acc_train: 0.787 acc_test: 0.803\n",
      "epoch:  2998 loss:  0.258 acc_train: 0.786 acc_test: 0.807\n",
      "epoch:  2999 loss:  0.333 acc_train: 0.787 acc_test: 0.807\n"
     ]
    }
   ],
   "source": [
    "acc_train_list = []\n",
    "acc_test_list = []\n",
    "loss_list = []\n",
    "for epoch in range(3000):\n",
    "    for x,y in train_dl:\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    with torch.no_grad():    \n",
    "        acc_train = ((model(train_x)> 0.5).type(torch.int32) == train_y).float().mean()\n",
    "        acc_test = ((model(test_x)> 0.5).type(torch.int32) == test_y).float().mean()\n",
    "        if epoch % 100 ==0:\n",
    "            loss_list.append(round(loss.data.item(), 3))\n",
    "            acc_train_list.append(round(acc_train.item(), 3))\n",
    "            acc_train_list.append(round(acc_test.item(), 3))\n",
    "        print(\"epoch: \", epoch, \"loss: \", round(loss.data.item(), 3), \"acc_train:\", round(acc_train.item(), 3),\"acc_test:\", round(acc_test.item(), 3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20dea49db00>]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_list,c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20dea52a438>]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_train_list,c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:morvanli] *",
   "language": "python",
   "name": "conda-env-morvanli-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
